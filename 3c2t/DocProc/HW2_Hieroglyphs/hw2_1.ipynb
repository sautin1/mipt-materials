{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# converts matrix (n, m), representing an image, into a vector (n*m, )\n",
    "def img_to_row(img, index):\n",
    "    data = img.flatten().astype(float)\n",
    "    return data\n",
    "\n",
    "# returns a matrix representing a directory of images (row = image, column = pixel number)\n",
    "def img_dir_to_matrix(root_path, extension = '', need_names = False):\n",
    "    index = 0\n",
    "    data = None\n",
    "    filenames = []\n",
    "    for (path, dirs, files) in os.walk(root_path):\n",
    "        for filename in files:\n",
    "            if (filename.endswith(extension)):\n",
    "                fullpath = os.path.join(path, filename)\n",
    "                img = cv2.imread(fullpath, 0)\n",
    "                row = img_to_row(img, index)\n",
    "                if index == 0:\n",
    "                    data = row.reshape(1, -1)\n",
    "                else:\n",
    "                    data = np.vstack([data, row])\n",
    "                if need_names:\n",
    "                    filenames.append(filename)\n",
    "#                 if index == 5:\n",
    "#                     return (data, filenames) if need_names else data\n",
    "                index += 1\n",
    "                if index % 5000 == 0:\n",
    "                    print index\n",
    "    return (data, filenames) if need_names else data\n",
    "\n",
    "def export_to_csv(path, array, head = None):\n",
    "    frame = pd.DataFrame(array, columns=head)\n",
    "    frame.to_csv(path, sep = ',', header = (head != None), index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_train_class(root_path, extension, class_value):\n",
    "    train = img_dir_to_matrix(root_path, extension)\n",
    "    train_y = np.array([class_value] * train.shape[0])\n",
    "    return (train, train_y)\n",
    "    \n",
    "def generate_train_sample(root_path, extension, class_true_dir, class_false_dir):\n",
    "    if not root_path.endswith('/'):\n",
    "        root_path += '/'\n",
    "    print 'Generating training sample for True class'\n",
    "    train_true, train_true_y = generate_train_class(root_path + class_true_dir, extension, 1)\n",
    "    print 'Generating training sample for False class'\n",
    "    train_false, train_false_y = generate_train_class(root_path + class_false_dir, extension, 0)\n",
    "    \n",
    "    print 'Merging samples'\n",
    "    trainX = np.concatenate((train_true, train_false), 0)\n",
    "    trainY = np.concatenate((train_true_y, train_false_y), 0)\n",
    "    return (trainX, trainY)\n",
    "\n",
    "def generate_test_sample(root_path, extension):\n",
    "    print 'Generating test sample'\n",
    "    if not root_path.endswith('/'):\n",
    "        root_path += '/'\n",
    "    (data, filenames) = img_dir_to_matrix(root_path, extension, need_names=True)\n",
    "    test_ids = np.array([int(filename.split('.')[0]) for filename in filenames])\n",
    "    frame = pd.DataFrame(data)\n",
    "    frame['id'] = test_ids\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training sample for True class\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "Generating training sample for False class\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "Merging samples\n",
      "CPU times: user 10min 54s, sys: 4min 16s, total: 15min 11s\n",
      "Wall time: 15min 13s\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "CPU times: user 3min 54s, sys: 1min 35s, total: 5min 29s\n",
      "Wall time: 5min 30s\n"
     ]
    }
   ],
   "source": [
    "%time trainX, trainY = generate_train_sample('data/train/', '.tif', 'Hieroglyph', 'Other')\n",
    "%time test_frame = generate_test_sample('data/test/', '.tif')\n",
    "\n",
    "export_to_csv('data/train/trainX.csv', trainX)\n",
    "export_to_csv('data/train/trainY.csv', trainY)\n",
    "test_frame.to_csv('data/test/testX.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.67 s, sys: 45.5 ms, total: 2.72 s\n",
      "Wall time: 2.72 s\n",
      "CPU times: user 4.7 ms, sys: 0 ns, total: 4.7 ms\n",
      "Wall time: 4.75 ms\n",
      "CPU times: user 1.18 s, sys: 12.6 ms, total: 1.19 s\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%time trainX = pd.read_csv('data/train/trainX.csv', header=None, dtype=np.float32).values.reshape((-1, 1, 20, 20))\n",
    "%time trainY = pd.read_csv('data/train/trainY.csv', header=None, dtype=np.int32).values.reshape((-1))\n",
    "%time test_info = pd.read_csv('data/test/testX.csv')\n",
    "testIds = test_info['id'].values.astype(int)\n",
    "testX = test_info.drop('id', 1).values.astype(np.float32).reshape((-1, 1, 20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import lasagne.layers as layers\n",
    "from lasagne.nonlinearities import softmax\n",
    "from lasagne.updates import adam\n",
    "from lasagne.updates import nesterov_momentum\n",
    "\n",
    "from nolearn.lasagne import NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers0 = [\n",
    "    # layer dealing with the input data\n",
    "    (layers.InputLayer, {'shape': (None, 1, 20, 20)}),\n",
    "\n",
    "    # first stage of our convolutional layers\n",
    "    (layers.Conv2DLayer, {'num_filters': 16, 'filter_size': 5}),\n",
    "    (layers.Conv2DLayer, {'num_filters': 32, 'filter_size': 3}),\n",
    "    (layers.Conv2DLayer, {'num_filters': 32, 'filter_size': 3}),\n",
    "    (layers.MaxPool2DLayer, {'pool_size': 2}),\n",
    "\n",
    "    # second stage of our convolutional layers\n",
    "    (layers.Conv2DLayer, {'num_filters': 32, 'filter_size': 3}),\n",
    "    (layers.Conv2DLayer, {'num_filters': 32, 'filter_size': 3}),\n",
    "    (layers.MaxPool2DLayer, {'pool_size': 2}),\n",
    "\n",
    "    # two dense layers with dropout\n",
    "    (layers.DenseLayer, {'num_units': 16}),\n",
    "    (layers.DropoutLayer, {}),\n",
    "    (layers.DenseLayer, {'num_units': 16}),\n",
    "\n",
    "    # the output layer\n",
    "    (layers.DenseLayer, {'num_units': 2, 'nonlinearity': softmax}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net0 = NeuralNet(\n",
    "    layers = layers0,\n",
    "    max_epochs = 100,\n",
    "    update = adam,\n",
    "    update_learning_rate = 0.01,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 33634 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "name        size        total    cap.Y    cap.X    cov.Y    cov.X\n",
      "----------  --------  -------  -------  -------  -------  -------\n",
      "input0      1x20x20       400   100.00   100.00   100.00   100.00\n",
      "conv2d1     16x16x16     4096   100.00   100.00    25.00    25.00\n",
      "conv2d2     32x14x14     6272    42.86    42.86    35.00    35.00\n",
      "conv2d3     32x12x12     4608    33.33    33.33    45.00    45.00\n",
      "maxpool2d4  32x6x6       1152    33.33    33.33    45.00    45.00\n",
      "conv2d5     32x4x4        512    46.15    46.15    65.00    65.00\n",
      "conv2d6     32x2x2        128    35.29    35.29    85.00    85.00\n",
      "maxpool2d7  32x1x1         32    35.29    35.29    85.00    85.00\n",
      "dense8      16             16   100.00   100.00   100.00   100.00\n",
      "dropout9    16             16   100.00   100.00   100.00   100.00\n",
      "dense10     16             16   100.00   100.00   100.00   100.00\n",
      "dense11     2               2   100.00   100.00   100.00   100.00\n",
      "\n",
      "Explanation\n",
      "    X, Y:    image dimensions\n",
      "    cap.:    learning capacity\n",
      "    cov.:    coverage of image\n",
      "    \u001b[35mmagenta\u001b[0m: capacity too low (<1/6)\n",
      "    \u001b[36mcyan\u001b[0m:    image coverage too high (>100%)\n",
      "    \u001b[31mred\u001b[0m:     capacity too low and coverage too high\n",
      "\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m0.41094\u001b[0m       \u001b[32m0.14988\u001b[0m      2.74187      0.95108  476.64s\n",
      "      2       \u001b[36m0.14234\u001b[0m       \u001b[32m0.10792\u001b[0m      1.31892      0.96416  476.24s\n",
      "      3       0.14303       \u001b[32m0.10022\u001b[0m      1.42711      0.96608  485.84s\n",
      "      4       0.14933       0.12353      1.20888      0.96119  482.96s\n",
      "      5       0.17040       0.14031      1.21448      0.95064  485.33s\n",
      "      6       0.16992       0.10459      1.62461      0.96838  496.25s\n",
      "      7       0.17933       0.14867      1.20623      0.95840  491.84s\n",
      "      8       0.35312       0.30796      1.14663      0.90197  510.80s\n",
      "      9       0.52910       0.45196      1.17067      0.86086  513.35s\n",
      "     10       0.66348       0.69361      0.95657      0.50391  530.98s\n",
      "     11       0.69417       0.69360      1.00082      0.50391  713.58s\n",
      "     12       0.69405       0.69358      1.00068      0.50391  716.19s\n",
      "     13       0.69400       0.69357      1.00062      0.50391  711.64s\n",
      "     14       0.69401       0.69358      1.00061      0.50391  720.08s\n",
      "     15       0.69405       0.69360      1.00064      0.50391  712.84s\n",
      "     16       0.69411       0.69361      1.00071      0.50391  713.67s\n",
      "     17       0.69418       0.69361      1.00082      0.50391  709.92s\n",
      "     18       0.69427       0.69361      1.00095      0.50391  711.85s\n",
      "     19       0.69435       0.69360      1.00108      0.50391  715.66s\n",
      "     20       0.69443       0.69360      1.00120      0.50391  719.83s\n",
      "     21       0.69450       0.69361      1.00129      0.50391  723.45s\n",
      "     22       0.69455       0.69361      1.00135      0.50391  703.01s\n",
      "     23       0.69458       0.69362      1.00138      0.50391  664.70s\n",
      "     24       0.69458       0.69362      1.00140      0.50391  654.54s\n",
      "     25       0.69458       0.69361      1.00139      0.50391  687.80s\n",
      "     26       0.69456       0.69361      1.00137      0.50391  714.48s\n",
      "     27       0.69454       0.69360      1.00134      0.50391  713.82s\n",
      "     28       0.69451       0.69360      1.00131      0.50391  724.66s\n",
      "     29       0.69447       0.69360      1.00126      0.50391  709.17s\n",
      "     30       0.69444       0.69360      1.00121      0.50391  699.70s\n",
      "     31       0.69440       0.69360      1.00115      0.50391  678.87s\n",
      "     32       0.69436       0.69360      1.00109      0.50391  666.18s\n",
      "     33       0.69431       0.69360      1.00102      0.50391  694.70s\n",
      "     34       0.69426       0.69361      1.00094      0.50391  665.38s\n",
      "     35       0.69421       0.69361      1.00087      0.50391  664.06s\n",
      "     36       0.69415       0.69360      1.00079      0.50391  675.25s\n",
      "     37       0.69409       0.69359      1.00073      0.50391  666.45s\n",
      "     38       0.69404       0.69357      1.00068      0.50391  674.30s\n",
      "     39       0.69399       0.69354      1.00065      0.50391  686.21s\n",
      "     40       0.69395       0.69351      1.00063      0.50391  698.22s\n",
      "     41       0.69391       0.69349      1.00061      0.50391  713.06s\n",
      "     42       0.69388       0.69347      1.00060      0.50391  692.56s\n",
      "     43       0.69387       0.69345      1.00059      0.50391  682.64s\n",
      "     44       0.69385       0.69344      1.00059      0.50391  695.53s\n",
      "     45       0.69384       0.69343      1.00060      0.50391  708.64s\n",
      "     46       0.69383       0.69341      1.00060      0.50391  691.72s\n",
      "     47       0.69382       0.69340      1.00060      0.50391  685.71s\n",
      "     48       0.69380       0.69338      1.00060      0.50391  685.24s\n",
      "     49       0.69379       0.69337      1.00061      0.50391  680.98s\n",
      "     50       0.69377       0.69335      1.00061      0.50391  698.09s\n",
      "     51       0.69376       0.69333      1.00062      0.50391  704.79s\n",
      "     52       0.69374       0.69331      1.00062      0.50391  711.77s\n",
      "     53       0.69373       0.69329      1.00063      0.50391  700.25s\n",
      "     54       0.69371       0.69327      1.00063      0.50391  700.95s\n",
      "     55       0.69369       0.69325      1.00063      0.50391  711.79s\n",
      "     56       0.69367       0.69323      1.00064      0.50391  700.94s\n",
      "     57       0.69365       0.69321      1.00063      0.50391  712.61s\n",
      "     58       0.69363       0.69320      1.00063      0.50391  693.81s\n",
      "     59       0.69361       0.69318      1.00062      0.50391  692.80s\n",
      "     60       0.69358       0.69316      1.00061      0.50391  705.27s\n",
      "     61       0.69356       0.69315      1.00058      0.50391  705.08s\n",
      "     62       0.69353       0.69315      1.00055      0.50391  707.05s\n",
      "     63       0.69350       0.69315      1.00051      0.49609  708.07s\n",
      "     64       0.69348       0.69315      1.00047      0.49609  698.99s\n",
      "     65       0.69346       0.69315      1.00044      0.49609  696.11s\n",
      "     66       0.69345       0.69315      1.00043      0.49609  684.34s\n",
      "     67       0.69345       0.69315      1.00043      0.49609  644.14s\n",
      "     68       0.69345       0.69315      1.00043      0.49609  502.96s\n",
      "     69       0.69345       0.69315      1.00043      0.49609  541.16s\n",
      "     70       0.69345       0.69315      1.00043      0.49609  537.92s\n",
      "     71       0.69345       0.69315      1.00043      0.49609  513.36s\n",
      "     72       0.69345       0.69315      1.00043      0.49609  498.43s\n",
      "     73       0.69345       0.69315      1.00043      0.49609  498.98s\n",
      "     74       0.69345       0.69315      1.00043      0.49609  503.10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x7f3a7b25bf90>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x7f3a7b25be90>,\n",
       "     check_input=True, custom_scores=None,\n",
       "     layers=[(<class 'lasagne.layers.input.InputLayer'>, {'shape': (None, 1, 20, 20)}), (<class 'lasagne.layers.conv.Conv2DLayer'>, {'filter_size': 5, 'num_filters': 16}), (<class 'lasagne.layers.conv.Conv2DLayer'>, {'filter_size': 3, 'num_filters': 32}), (<class 'lasagne.layers.conv.Conv2DLayer'>, {'fil....layers.dense.DenseLayer'>, {'num_units': 2, 'nonlinearity': <function softmax at 0x7f3a7be1b320>})],\n",
       "     loss=None, max_epochs=100, more_params={},\n",
       "     objective=<function objective at 0x7f3a7b25d668>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0x7f3a7bcaa758>,\n",
       "     on_batch_finished=[],\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog instance at 0x7f3a80855248>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x7f3a80891638>],\n",
       "     regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x7f3a7b25bfd0>,\n",
       "     update=<function adam at 0x7f3a7bcaf2a8>, update_learning_rate=0.01,\n",
       "     use_label_encoder=False, verbose=2,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_indices = np.arange(np.size(trainY))\n",
    "np.random.shuffle(random_indices)\n",
    "# print trainY[random_indices]\n",
    "net0.fit(trainX[random_indices], trainY[random_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "No progress in learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
