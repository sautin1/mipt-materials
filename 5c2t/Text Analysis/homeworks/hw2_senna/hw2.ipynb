{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 3 [10 баллов] \n",
    "# До 30.04.18 23:59\n",
    "\n",
    "Задание выполняется в группе (1-4 человека). В случае использования какого-либо строннего источника информации обязательно дайте на него ссылку (поскольку другие тоже могут на него наткнуться). Плагиат наказывается нулём баллов за задание и предвзятым отношением в будущем.\n",
    "\n",
    "Не все части обязательны для выполнения, однако вы можете быть дополнительно оштрафованы за небрежное за выполнение одной или двух частей вместо четырех.\n",
    "\n",
    "При возниконовении проблем с выполнением задания обращайтесь с вопросами к преподавателю. Поэтому настоятельно рекомендуется выполнять задание заранее, оставив запас времени на всевозможные технические проблемы. Если вы начали читать условие в последний вечер и не успели из-за проблем с установкой какой-либо библиотеки — это ваши проблемы.\n",
    "\n",
    "\n",
    "Результат выполнения задания — это отчёт в формате html на основе Jupyter Notebook. Нормальный отчёт должен включать в себя:\n",
    "* Краткую постановку задачи и формулировку задания\n",
    "* Описание **минимума** необходимой теории и/или описание используемых инструментов - не стоит переписывать лекции или Википедию\n",
    "* Подробный пошаговый рассказ о проделанной работе\n",
    "* Аккуратно оформленные результаты\n",
    "* **Внятные выводы** – не стоит относится к домашнему заданию как к последовательности сугубо технических шагов, а стоит относится скорее как к небольшому практическому исследованию, у которого есть своя цель и свое назначение.\n",
    "\n",
    "Небрежное его оформление отчета существенно отразится на итоговой оценке. Весь код из отчёта должен быть воспроизводимым, если для этого нужны какие-то дополнительные действия, установленные модули и т.п. — всё это должно быть прописано в тексте в явном виде.\n",
    "\n",
    "Сдача отчетов осуществляется через систему AnyTask.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Использование архитектуры SENNA для определения части речи\n",
    "\n",
    "Домашнее задание написано по мотивам работы R. Collobert:\n",
    "\n",
    "**Collobert, Ronan, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. \"Natural language processing (almost) from scratch.\" Journal of Machine Learning Research 12, no. Aug (2011): 2493-2537.**\n",
    "\n",
    "В этом домашнем задании вам предстоит самостоятельно разработать архитектуру SENNA для определения части речи. \n",
    "SENNA – это простая архитектура нейронной сети, позволяющая достигнуть state-of-the-art результатов в нескольких задачах обработки текстов.  \n",
    "\n",
    "Использование SENNA для определения части речи предполагает, что задача определения части речи для данного слова формулируется как задача классификации: пусть в размеченном корпусе всего $|T|$ (= tagset) различных тегов частей речи, тогда каждое слово $w$ относится к одному из $T$ классов. Для каждого слова из обучающих данных формируется собственный вектор признаков. Нейронная сеть обучается по всем векторам признаков для слов из обучающего множества. \n",
    "\n",
    "Подход к решению задачи классификации представлен в оригинальной статье на рис. 1 (Figure 1: Window approach network). Он состоит из следующих шагов (раздел 3.3.1):\n",
    "1. Каждое слово представляется эмбеддингом: $w_i \\rightarrow LT_{w^i}$, размерность эмбеддинга - $d$;\n",
    "2. Для каждого слова формируется окно длины $k$ из $(k-1)/2$ соседних слов слева от данного слова  и $(k-1)/2$ соседних слов справа от данного слова, $k$ – нечетное. \n",
    "3. Для каждого слова формируется вектор признаков, состоящий из конкатенированных эмбеддингов слов из левого окна, данного слова и слов из правого окна. Итоговая размерность вектора признаков – $d \\times k$. Именно этот вектор подается на вход нейронной сети;\n",
    "4. Обучается нейронная сеть, имеющая один скрытый слой с $n_h$ нейроннами и нелинейной функцией активации $\\theta$;\n",
    "5. На выходном слое нейронной сети решается задача классификации на |T| классов, то есть, определяется часть речи для каждого слова. \n",
    "\n",
    "Если для слова невозможно найти $(k-1)/2$ соседних слов слева от данного слова  и $(k-1)/2$ соседних слов справа от данного слова – используется padding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные\n",
    "1. Открытый корпус: https://github.com/dialogue-evaluation/morphoRuEval-2017/blob/master/OpenCorpora_Texts.rar\n",
    "2. Предобученные эмбеддинги Facebook: https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.ru.vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1 [2 балла] Подготовка данных\n",
    "1. Прочитайте размеченные данные Открытого корпуса, используя nltk.corpus.reader.conll.ConllCorpusReader\n",
    "2. Посчитайте количество предложений и число тегов частей речи;\n",
    "3. Сформируйте тестовое и обучающее множество: первые 3/4 данных – обучающее множество;\n",
    "\n",
    "Для каждого слова:\n",
    "1. Определите его окно (слова слева и справа) размера $k$;\n",
    "2. Сформируйте его вектор признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читаем корпус, считаем простейшие статистики.\n",
    "\n",
    "from nltk.corpus.reader.conll import ConllCorpusReader\n",
    "\n",
    "\n",
    "data_reader = ConllCorpusReader('./data', fileids='unamb_sent_14_6.conllu',\n",
    "                                columntypes=['ignore', 'words', 'ignore', 'pos', 'chunk'])\n",
    "sentences = list(data_reader.iob_sents())\n",
    "pos_tags = [pos for sentence in sentences for word, pos, chunk in sentence]\n",
    "pos_tag_set = set(pos_tags)\n",
    "pos_tag_to_num = {tag: num for num, tag in enumerate(pos_tag_set)}\n",
    "\n",
    "print(f'Количество предложений: {len(sentences)}')\n",
    "print(f'Количество различных POS-тегов: {len(pos_tag_to_num)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делим предложения из корпуса на две группы: для обучающей и тестовой выборки.\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "TRAIN_PERCENT = 0.75\n",
    "\n",
    "random.shuffle(sentences)\n",
    "sentences_train_size = int(TRAIN_PERCENT * len(sentences))\n",
    "sentences_train, sentences_test = sentences[:sentences_train_size], sentences[sentences_train_size:]\n",
    "print(f'Количество предложений в обучающей выборке: {len(sentences_train)}')\n",
    "print(f'Количество предложений в тестовой выборке: {len(sentences_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Загружаем fasttext модель, ограничиваясь первыми limit словами.\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "fasttext_model = KeyedVectors.load_word2vec_format('wiki.ru.vec', limit=999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Создаем функцию, формирующую вектор признаков для слова\n",
    "# на основе его индекса в языковой модели и индексов его соседей.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_feature_vector(sentence, window_size, word_idx, fasttext_model):\n",
    "    window_half_size = (window_size - 1) // 2\n",
    "    features = np.zeros((window_size,))\n",
    "    idx_start = word_idx - window_half_size\n",
    "    for idx in range(max(idx_start, 0), min(word_idx + window_half_size + 1, len(sentence))):\n",
    "        word = sentence[idx][0].lower()\n",
    "        word_in_vocab = fasttext_model.vocab.get(word, None)\n",
    "        if word_in_vocab is not None:\n",
    "            features[idx - idx_start] = word_in_vocab.index\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Создаем класс для набора параметров эксперимента, задаем параметры первого эксперимента.\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "ExperimentParams = namedtuple('ExperimentParams', ('window_size',\n",
    "                                                   'hidden_layer_units',\n",
    "                                                   'hidden_layer_activation'))\n",
    "Metrics = namedtuple('Metrics', ('acc', 'val_acc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_Начиная отсюда и до части 4, код будет зависеть от конкретных параметров эксперимента._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формируем обучающую и тестовую выборки.\n",
    "# Т.к. при обучении модели будет использоваться categorical_crossentropy loss,\n",
    "# кодируем все метки классов с помощью one-hot-encoding.\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def prepare_data(sentences, window_size, fasttext_model, pos_tag_to_num):\n",
    "    features = [make_feature_vector(sentence, window_size, idx, fasttext_model)\n",
    "                for sentence in sentences for idx in range(len(sentence))]\n",
    "    tags = [to_categorical(pos_tag_to_num[tag], len(pos_tag_to_num))\n",
    "            for sentence in sentences for _, tag, _ in sentence]\n",
    "    return np.array(features), np.array(tags)\n",
    "\n",
    "\n",
    "PARAMS_INIT = ExperimentParams(window_size=5, hidden_layer_units=256, hidden_layer_activation='tanh')\n",
    "words_train, tags_train = prepare_data(sentences_train, PARAMS_INIT.window_size, fasttext_model, pos_tag_to_num)\n",
    "words_test, tags_test = prepare_data(sentences_test, PARAMS_INIT.window_size, fasttext_model, pos_tag_to_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 2 [4 баллов] Архитектура нейронной сети\n",
    "\n",
    "Архитектура нейронной сети состоит из следующих слов:\n",
    "1. Входной слой: нейронная сеть получает на вход вектор признаков, состоящий из $k$ конкатенированных эмбеддингов;/\n",
    "2. Скрытый слой: $n_h$ нейронов и нелинейная функция активации $\\theta$;\n",
    "3. Выходной слой:  $|T|$ нейронов для итоговой классификации.\n",
    "\n",
    "Обучите нейронную сеть на обучающих данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем модель классификатора, компилируем ее и выводим краткое описание.\n",
    "\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def create_classifier_model(input_shape, hidden_layer_units, hidden_layer_activation,\n",
    "                            class_count, fasttext_model):\n",
    "    # Пользуемся функционалом gensim.KeyedVectors для создания слоя Embedding.\n",
    "    # и инициализации его весами\n",
    "    embedding_layer = fasttext_model.get_keras_embedding()\n",
    "    # Также отмечаем, что эти веса не надо менять при обучении сети.\n",
    "    embedding_layer.trainable = False\n",
    "\n",
    "    input_layer = Input(input_shape)\n",
    "    embedding = embedding_layer(input_layer)\n",
    "    flattened = Flatten()(embedding)\n",
    "    hidden = Dense(hidden_layer_units, activation=hidden_layer_activation)(flattened)\n",
    "    output = Dense(class_count, activation='softmax')(hidden)\n",
    "    return Model(inputs=[input_layer], outputs=[output])\n",
    "\n",
    "\n",
    "classifier = create_classifier_model((PARAMS_INIT.window_size,),\n",
    "                                     PARAMS_INIT.hidden_layer_units,\n",
    "                                     PARAMS_INIT.hidden_layer_activation,\n",
    "                                     len(pos_tag_to_num),\n",
    "                                     fasttext_model)\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Обучаем классификатор.\n",
    "# Сохраняем лог обучения и веса модели.\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def make_experiment_name(params):\n",
    "    return ','.join([f'window_{params.window_size}',\n",
    "                     f'units_{params.hidden_layer_units}',\n",
    "                     f'act_{params.hidden_layer_activation}'])\n",
    "\n",
    "\n",
    "def train_model(model, data_train, data_validation, epochs, name='main', verbose=1):\n",
    "    history = classifier.fit(data_train[0], data_train[1], epochs=epochs, validation_data=data_validation, verbose=verbose)\n",
    "    with open(f'logs/{name}.json', 'w') as fout:\n",
    "        json.dump(history.history, fout, sort_keys=True, indent=4)\n",
    "    classifier.save_weights(f'weights/{name}')\n",
    "    return Metrics(acc=history.history['acc'][-1], val_acc=history.history['val_acc'][-1])\n",
    "\n",
    "\n",
    "metrics = train_model(classifier, (words_train, tags_train), (words_test, tags_test),\n",
    "                      epochs=10, name=make_experiment_name(PARAMS_INIT))\n",
    "RESULTS = {PARAMS_INIT: metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 3 [1 балл] Оценка качества\n",
    "\n",
    "Протестируйте нейронную сеть на тестовых данных. Используйте accuracy для оценки качества модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В еще одном прогоне тестовых данных через модель нет необходимости,\n",
    "# т.к. accuracy на нем уже был посчитан в конце последней эпохи.\n",
    "# Тем не менее, продемонстрирую, что я умею это делать. :)\n",
    "\n",
    "metrics_values = classifier.evaluate(words_test, tags_test)\n",
    "for metric_name, metric_value in zip(classifier.metrics_names, metrics_values):\n",
    "    print(f'{metric_name}: {metric_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 4 [1 балл] Оптимизация гиперпарметров\n",
    "\n",
    "В эксперименте участвуют следующие гиперпараметры:\n",
    "* $k$ – размер окна;\n",
    "* $n_h$ – число нейронов на скрытом слое;\n",
    "* $\\theta$ – вид функции активации.\n",
    "\n",
    "Оцените их влияние на качество модели. Как увеличение окна или числа нейронов влияет на итоговый показатель качества? Зависит ли итоговый показатель качества от функции активации на скрытом слое? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Перебираем параметры по сетке.\n",
    "import itertools\n",
    "\n",
    "window_sizes = [1, 3, 5, 7]\n",
    "hidden_layer_units_counts = [64, 128, 256, 512]\n",
    "hidden_layer_activations = ['relu', 'tanh']\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    words_train, tags_train = prepare_data(sentences_train, window_size, fasttext_model, pos_tag_to_num)\n",
    "    words_test, tags_test = prepare_data(sentences_test, window_size, fasttext_model, pos_tag_to_num)\n",
    "\n",
    "    for hidden_layer_units, hidden_layer_activation in itertools.product(hidden_layer_units_counts,\n",
    "                                                                         hidden_layer_activations):\n",
    "        params = ExperimentParams(window_size, hidden_layer_units, hidden_layer_activation)\n",
    "        if params == PARAMS_INIT:\n",
    "            continue\n",
    "        classifier = create_classifier_model((window_size,),\n",
    "                                             hidden_layer_units,\n",
    "                                             hidden_layer_activation,\n",
    "                                             len(pos_tag_to_num),\n",
    "                                             fasttext_model)\n",
    "        classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        metrics = train_model(classifier, (words_train, tags_train), (words_test, tags_test),\n",
    "                              epochs=10, name=make_experiment_name(params), verbose=0)\n",
    "        RESULTS[params] = metrics\n",
    "        print(params)\n",
    "        print(metrics)\n",
    "        print()\n",
    "        K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 5 [2 балла] Анализ ошибок\n",
    "1. Привидите примеры из тестового множества, на которых нейронная сеть ошибается. Объясните, почему возникают ошибки.\n",
    "2. Протестируйте нейронную сеть на произвольном предложении (не из тестовых данных). Возникают ли ошибки? Почему?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
