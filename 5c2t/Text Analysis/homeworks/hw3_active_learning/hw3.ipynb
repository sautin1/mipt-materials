{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 3 [10 баллов] \n",
    "# До 16.05.18 23:59\n",
    "\n",
    "Задание выполняется в группе (1-4 человека). В случае использования какого-либо строннего источника информации обязательно дайте на него ссылку (поскольку другие тоже могут на него наткнуться). Плагиат наказывается нулём баллов за задание и предвзятым отношением в будущем.\n",
    "\n",
    "Не все части обязательны для выполнения, однако вы можете быть дополнительно оштрафованы за небрежное за выполнение одной или двух частей вместо четырех.\n",
    "\n",
    "При возниконовении проблем с выполнением задания обращайтесь с вопросами к преподавателю. Поэтому настоятельно рекомендуется выполнять задание заранее, оставив запас времени на всевозможные технические проблемы. Если вы начали читать условие в последний вечер и не успели из-за проблем с установкой какой-либо библиотеки — это ваши проблемы.\n",
    "\n",
    "\n",
    "Результат выполнения задания — это отчёт в формате html на основе Jupyter Notebook. Нормальный отчёт должен включать в себя:\n",
    "* Краткую постановку задачи и формулировку задания\n",
    "* Описание **минимума** необходимой теории и/или описание используемых инструментов - не стоит переписывать лекции или Википедию\n",
    "* Подробный пошаговый рассказ о проделанной работе\n",
    "* Аккуратно оформленные результаты\n",
    "* **Внятные выводы** – не стоит относится к домашнему заданию как к последовательности сугубо технических шагов, а стоит относится скорее как к небольшому практическому исследованию, у которого есть своя цель и свое назначение.\n",
    "\n",
    "Небрежное его оформление отчета существенно отразится на итоговой оценке. Весь код из отчёта должен быть воспроизводимым, если для этого нужны какие-то дополнительные действия, установленные модули и т.п. — всё это должно быть прописано в тексте в явном виде.\n",
    "\n",
    "Сдача отчетов осуществляется через систему AnyTask.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация текстов с активным обучением\n",
    "\n",
    "\n",
    "Зададимся простой задачей классификации текстов: например, классификацией отзывов на банки по тональности. Эта задача решается с достаточно высокими показателями качества с использованием стандартных алгоритмов классификации, например, сверточных нейронных сетей: корпус состоит из достаточного количества документов, чтобы сверточная сеть хорошо обучилась. Однако возникает естественный вопрос: действительно ли все документы нужны для того, чтобы достичь таких высоких показателей качества (или сопоставимых с ними). Парадигма активного обучения поможет вам ответить на этот вопрос."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Предобработка данных [2 балла]\n",
    "\n",
    "Коллекция отзывов хранится в файле banki_responses (https://www.dropbox.com/s/ol3ux3ibr6rd5ke/banki_responses.json.bz2?dl=0). Одна строчка в этом файле соответствует одному json-словарю. Из этого словаря вам понадобятся два значения по ключам text и rating -- текст отзыва и его оценка по шкале от 1 до 5.   \n",
    "\n",
    "Считайте файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Напишем функцию для разбиения предложения на слова (с удалением знаков пунктуации и\n",
    "# приведением слов к нижнему регистру).\n",
    "# Также напишем функцию для чтения коллекции отзывов из файла (игнорируя те, у которых не задана оценка).\n",
    "# Каждый отзыв будем представлять словарем с полями:\n",
    "# text -- текст отзыва,\n",
    "# words -- слова отзыва,\n",
    "# rating -- оценка в отзыве.\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def split_into_words(sentence):\n",
    "    regexp = \"[^а-яА-ЯёЁa-zA-Z]\"\n",
    "    sentence = re.sub(regexp, \" \", sentence)\n",
    "    return sentence.lower().split()\n",
    "\n",
    "\n",
    "def read_data(path='./data/banki_responses.json'):\n",
    "    reviews = []\n",
    "    with open(path, 'r') as fin:\n",
    "        for line in fin:\n",
    "            reviews.append(json.loads(line.strip()))\n",
    "    return [{'text': review['text'],\n",
    "             'words': split_into_words(review['text']),\n",
    "             'rating': review['rating_grade']}\n",
    "            for review in reviews\n",
    "            if review['rating_grade'] is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество отзывов: 136189.\n",
      "Wall time: 52.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "reviews = read_data()\n",
    "print(f'Количество отзывов: {len(reviews)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитайте, каких отзывов больше: положительных (с оценкой 5) или отрицательных (с оценкой 1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество положительных отзывов: 26715.\n",
      "Количество отрицательных отзывов: 72307.\n",
      "Отрицательных отзывов больше.\n"
     ]
    }
   ],
   "source": [
    "reviews_positive_count = sum(1 for review in reviews if review['rating'] == 5)\n",
    "reviews_negative_count = sum(1 for review in reviews if review['rating'] == 1)\n",
    "print(f'Количество положительных отзывов: {reviews_positive_count}.')\n",
    "print(f'Количество отрицательных отзывов: {reviews_negative_count}.')\n",
    "print(('Положительных' if reviews_negative_count < reviews_positive_count else 'Отрицательных')\n",
    "      + ' отзывов больше.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите предварительную обработку данных: удалите слишком короткие и слишком длинные тексты (пороги на длину определите самостоятельно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEKCAYAAADq59mMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29b7de9ef60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Оценим распределение количества слов в отзывах, построив гистограмму.\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "plt.hist([len(review['words']) for review in reviews], 2000, facecolor='green', alpha=0.7)\n",
    "plt.xlabel('Review length (in words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Банк закрыт, на дверях замок.\n",
      "5: Банк хороший, обслуживаением доволен!\n",
      "5: Брал кредит все супер!\n",
      "5: Очень хороший банк! Вежливые сотрудники!\n",
      "1: Делают карту VISA почти 3 недели...\n",
      "3: Охранники у банка слишком злые.\n"
     ]
    }
   ],
   "source": [
    "# Оценим адекватность самых коротких отзывов,\n",
    "# чтобы выявить нижнюю границу на количество слов в отзыве выборки.\n",
    "\n",
    "for review in reviews:\n",
    "    if len(review['words']) <= 5:\n",
    "        print(f'{review[\"rating\"]}: {review[\"text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество отзывов после фильтрации по количеству слов: 134911.\n"
     ]
    }
   ],
   "source": [
    "# Из гистограммы видно, что отзывы с количеством слов >= 1000 составляю малые долю всех отзывов.\n",
    "# Поставим соответствующее ограничение сверху (1000) на количество слов в отзыве.\n",
    "\n",
    "# Из предыдущего примера можно видеть, что самые короткие отзывы состоят из 4-5 слов,\n",
    "# но при этом являются вполне адекватными, поэтому на текущем этапе причин для их удаления из выборки я не вижу.\n",
    "\n",
    "# Однако, заглядывая вперед, можно увидеть, что сверточной сети придется работать с \"изображениями\" большого разрешения:\n",
    "# (review_len, embedding_len), где\n",
    "# review_len -- количество слов в отзыве, а\n",
    "# embedding_len -- размер выходного вектора эмбеддинга.\n",
    "# Посему хорошо бы уменьшить разрешение на входе сетки, а также заметить,\n",
    "# что в ней будут использоваться pooling-слои, также понижающие размерность изображения.\n",
    "# По этой причине исходная длина отзыва (в словах) должна быть не меньше чем downsample factor сети.\n",
    "# Поставим соответствующее ограничение снизу (16) на количество слов в отзыве.\n",
    "\n",
    "reviews = [review for review in reviews if 16 <= len(review['words']) < 1000]\n",
    "print(f'Количество отзывов после фильтрации по количеству слов: {len(reviews)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбейте данные на обучающее ($train$) и тестовое ($test$) множество случайным образом в отношеннии 3:1 (или любом другом отношении, которое покажется вам разумным).\n",
    "Задача классификации сформулирована так: по каждому отзыву определить его оценку (т.е. классификация на 5 классов). Признаками для классификации выступают слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В обучающей выборке 101183 отзывов.\n",
      "В тестовой выборке 33728 отзывов.\n"
     ]
    }
   ],
   "source": [
    "# Разобьем коллекцию отзывов на обучающую и тестовую выборки.\n",
    "\n",
    "import random\n",
    "\n",
    "TRAIN_PERCENT = 0.75\n",
    "\n",
    "reviews_indices_train = random.sample(list(range(len(reviews))),\n",
    "                                      int(TRAIN_PERCENT * len(reviews)))\n",
    "reviews_indices_test = list(set(range(len(reviews))) - set(reviews_indices_train))\n",
    "\n",
    "reviews_train = [reviews[idx] for idx in reviews_indices_train]\n",
    "reviews_test = [reviews[idx] for idx in reviews_indices_test]\n",
    "\n",
    "print(f'В обучающей выборке {len(reviews_train)} отзывов.')\n",
    "print(f'В тестовой выборке {len(reviews_test)} отзывов.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASautin\\AppData\\Local\\Continuum\\Anaconda2\\envs\\con3.6\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Загружаем fasttext модель, ограничиваясь первыми limit словами.\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "fasttext_model = KeyedVectors.load_word2vec_format('../hw2_senna/wiki.ru.vec', limit=999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Напишем функцию, превращающую отзыв (словарь) в тензор, который можно будет подавать на вход сети.\n",
    "# Для этого отображаем слова отзыва в соответствующие им индексы в языковой модели fasttext.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_tensor_from_review(review, fasttext_model):\n",
    "    words = review['words']\n",
    "    tensor = np.zeros((len(words),))\n",
    "    for idx, word in enumerate(words):\n",
    "        word_in_vocab = fasttext_model.vocab.get(word, None)\n",
    "        if word_in_vocab is not None:\n",
    "            tensor[idx] = word_in_vocab.index\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Преобразуем отзывы обучающей и тестовой выборок в тензоры и отсортируем их по количеству слов.\n",
    "# Сортировка полезна, чтобы избавиться от длинных паддингов при формировании и выравнивании батчей.\n",
    "# Т.к. при обучении модели будет использоваться categorical_crossentropy loss,\n",
    "# кодируем все метки классов с помощью one-hot-encoding.\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "CLASS_COUNT = 5\n",
    "\n",
    "tensors_train = [make_tensor_from_review(review, fasttext_model) for review in reviews_train]\n",
    "tensors_test = [make_tensor_from_review(review, fasttext_model) for review in reviews_test]\n",
    "\n",
    "indices_train = sorted(list(range(len(tensors_train))), key=lambda idx: tensors_train[idx].shape[0])\n",
    "indices_test = sorted(list(range(len(tensors_test))), key=lambda idx: tensors_test[idx].shape[0])\n",
    "\n",
    "tensors_train_sorted = [tensors_train[idx] for idx in indices_train]\n",
    "tensors_test_sorted = [tensors_test[idx] for idx in indices_test]\n",
    "ratings_train_sorted = [to_categorical(reviews_train[idx]['rating'] - 1, CLASS_COUNT)\n",
    "                        for idx in indices_train]\n",
    "ratings_test_sorted = [to_categorical(reviews_test[idx]['rating'] - 1, CLASS_COUNT)\n",
    "                       for idx in indices_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество батчей в обучающей выборке: 6324.\n",
      "Количество батчей в тестовой выборке: 2108.\n"
     ]
    }
   ],
   "source": [
    "# Объединяем тензоры и метки каждой из выборок в батчи и выравниваем тензоры внутри батча.\n",
    "# Т.к. мы собираем батчи вручную, обучение нужно будет делать с помощью метода fit_generator,\n",
    "# посему создаем бесконечные итераторы по батчам обеих выборок.\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def iterate_over_batches(batches_x, batches_y, randomize=False):\n",
    "    indices = list(range(len(batches_x)))\n",
    "    while True:\n",
    "        if randomize:\n",
    "            random.shuffle(indices)\n",
    "        for idx in indices:\n",
    "            yield batches_x[idx], batches_y[idx]\n",
    "\n",
    "            \n",
    "def make_batches(xs, ys, batch_size=BATCH_SIZE):\n",
    "    batches_x = [xs[idx:idx + batch_size] for idx in range(0, len(xs), batch_size)]\n",
    "    batches_x = [pad_sequences(batch, padding='post') for batch in batches_x]\n",
    "    batches_y = [np.array(ys[idx:idx + batch_size]) for idx in range(0, len(ys), batch_size)]\n",
    "    return batches_x, batches_y\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "batches_train_x, batches_train_y = make_batches(tensors_train_sorted, ratings_train_sorted, BATCH_SIZE)\n",
    "batches_test_x, batches_test_y = make_batches(tensors_test_sorted, ratings_test_sorted, BATCH_SIZE)\n",
    "\n",
    "gen_train = iterate_over_batches(batches_train_x, batches_train_y, randomize=True)\n",
    "gen_test = iterate_over_batches(batches_test_x, batches_test_y, randomize=False)\n",
    "\n",
    "print(f'Количество батчей в обучающей выборке: {len(batches_train_x)}.')\n",
    "print(f'Количество батчей в тестовой выборке: {len(batches_test_x)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Baseline [4 балла]\n",
    "\n",
    "Получите baseline классификации: в идеале, используйте сверточную нейронную сеть (слой эмбеддингов + свертка + субдескритизация). Число и размерность фильтров определите самостоятельно, так же как и использование регуляризаторов (dropout / batch norm) и их параметров. Так же самостоятельно (но обосновано) решите, использовать ли вам предобученные эмбеддинги или нет и проводить ли вам лемматизацию или нет. \n",
    "\n",
    "Обучите сеть на обучающем множестве и протестируйте на тестовом. Зафиксируйте baseline.\n",
    "\n",
    "Если совсем трудно или вычисления занимают слишком много времени, используйте любой другой известный и симпатичный вам алгоритм классификации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsample factor: 16\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 300)         299999700 \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, None, 300)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 10, None, 150)     100       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, None, 75)      1820      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, None, 75)      5792      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, None, 75)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, None, 75)      528       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, None, 75)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, None, 75)      4640      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, None, 75)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, None, 75)      528       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 16, None, 75)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, None, 38)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, None, 38)      64        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, None, 38)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 64, None, 38)      9280      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64, None, 38)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, None, 38)      2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32, None, 38)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 64, None, 38)      18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 64, None, 38)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, None, 38)      2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 32, None, 38)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, None, 19)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, None, 19)      128       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, None, 19)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 128, None, 19)     36992     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 128, None, 19)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 64, None, 19)      8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 64, None, 19)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 128, None, 19)     73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 128, None, 19)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 64, None, 19)      8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 64, None, 19)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 5, None, 19)       325       \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 300,172,921\n",
      "Trainable params: 173,125\n",
      "Non-trainable params: 299,999,796\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Создадим модель сети с архитектурой DarkNet, которая хорошо себя показывает на\n",
    "# задачах классификации и детекции при работе с изображениями.\n",
    "# Компилируем модель классификатора и выводим ее краткое описание.\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, GlobalAveragePooling2D, MaxPool2D, Dropout\n",
    "from keras.layers import Reshape, BatchNormalization, LeakyReLU, Activation\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "class DarknetBlock:\n",
    "    def __init__(self, filters1, filters3, strides=(1, 1)):\n",
    "        self._filters1 = filters1\n",
    "        self._filters3 = filters3\n",
    "        self._strides = strides\n",
    "\n",
    "    def __call__(self, input_tensor, *args, **kwargs):\n",
    "        x = input_tensor\n",
    "        for filters, size, strides in [(self._filters3, (3, 3), self._strides),\n",
    "                                       (self._filters1, (1, 1), (1, 1))]:\n",
    "            x = Conv2D(filters, size, strides=strides, padding='same',\n",
    "                       kernel_initializer='he_normal', kernel_regularizer=l2(1e-10))(x)\n",
    "            x = LeakyReLU(alpha=0.05)(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def create_classifier_model(input_shape, class_count, fasttext_model, filter_counts=None):\n",
    "    filter_counts = filter_counts or [16, 32, 64]\n",
    "    downsample_factor = 1\n",
    "\n",
    "    # Пользуемся функционалом gensim.KeyedVectors для создания слоя Embedding.\n",
    "    # и инициализации его весами.\n",
    "    embedding_layer = fasttext_model.get_keras_embedding()\n",
    "    # Также отмечаем, что эти веса не надо менять при обучении сети.\n",
    "    embedding_layer.trainable = False\n",
    "\n",
    "    input_layer = Input(input_shape)\n",
    "    embedding = embedding_layer(input_layer)\n",
    "    # Добавляем ось каналов в тензор.\n",
    "    reshaped = Reshape((1, -1, fasttext_model.vector_size))(embedding)\n",
    "    x = reshaped\n",
    "    # Понижаем размерность исходного изображения.\n",
    "    x = Conv2D(10, (3, 3), strides=(2, 2), padding='same',\n",
    "               kernel_initializer='he_normal', kernel_regularizer=l2(1e-10))(x)\n",
    "    downsample_factor *= 2\n",
    "    x = Conv2D(20, (3, 3), strides=(2, 2), padding='same',\n",
    "               kernel_initializer='he_normal', kernel_regularizer=l2(1e-10))(x)\n",
    "    downsample_factor *= 2\n",
    "    # Сверточная сеть.\n",
    "    for idx, filter_count in enumerate(filter_counts):\n",
    "        x = DarknetBlock(filter_count, filter_count * 2)(x)\n",
    "        x = DarknetBlock(filter_count, filter_count * 2)(x)\n",
    "        if idx != len(filter_counts) - 1:\n",
    "            x = MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
    "            downsample_factor *= 2\n",
    "            x = BatchNormalization(axis=1)(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "    # Классификация.\n",
    "    x = Conv2D(class_count, (1, 1), padding='same')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    output = Activation('softmax')(x)\n",
    "    return Model(inputs=[input_layer], outputs=[output]), downsample_factor\n",
    "\n",
    "\n",
    "classifier, downsample_factor = create_classifier_model((None,), CLASS_COUNT, fasttext_model)\n",
    "print(f'Downsample factor: {downsample_factor}')\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1054/1054 [==============================] - 135s 129ms/step - loss: 1.1140 - acc: 0.6105 - val_loss: 1.0412 - val_acc: 0.6368\n",
      "Epoch 2/50\n",
      "1054/1054 [==============================] - 105s 99ms/step - loss: 0.9970 - acc: 0.6570 - val_loss: 1.3660 - val_acc: 0.6002\n",
      "Epoch 3/50\n",
      "1054/1054 [==============================] - 105s 100ms/step - loss: 0.9771 - acc: 0.6586 - val_loss: 1.0299 - val_acc: 0.6422\n",
      "Epoch 4/50\n",
      "1054/1054 [==============================] - 104s 99ms/step - loss: 0.9678 - acc: 0.6610 - val_loss: 1.0874 - val_acc: 0.6490\n",
      "Epoch 5/50\n",
      "1054/1054 [==============================] - 113s 107ms/step - loss: 0.9383 - acc: 0.6750 - val_loss: 0.9952 - val_acc: 0.6540\n",
      "Epoch 6/50\n",
      "1054/1054 [==============================] - 103s 97ms/step - loss: 0.9332 - acc: 0.6747 - val_loss: 0.9194 - val_acc: 0.6788\n",
      "Epoch 7/50\n",
      "1054/1054 [==============================] - 108s 103ms/step - loss: 0.9297 - acc: 0.6699 - val_loss: 0.9456 - val_acc: 0.6678\n",
      "Epoch 8/50\n",
      "1054/1054 [==============================] - 103s 98ms/step - loss: 0.9173 - acc: 0.6775 - val_loss: 0.9637 - val_acc: 0.6770\n",
      "Epoch 9/50\n",
      "1054/1054 [==============================] - 99s 94ms/step - loss: 0.9262 - acc: 0.6727 - val_loss: 3.3367 - val_acc: 0.2447\n",
      "Epoch 10/50\n",
      "1054/1054 [==============================] - 101s 96ms/step - loss: 0.9166 - acc: 0.6748 - val_loss: 1.1367 - val_acc: 0.6300\n",
      "Epoch 11/50\n",
      "1054/1054 [==============================] - 102s 97ms/step - loss: 0.9145 - acc: 0.6775 - val_loss: 0.9418 - val_acc: 0.6687\n",
      "Epoch 12/50\n",
      "1053/1054 [============================>.] - ETA: 0s - loss: 0.9088 - acc: 0.6803\n",
      "Epoch 00012: reducing learning rate to 0.0005000000237487257.\n",
      "1054/1054 [==============================] - 101s 96ms/step - loss: 0.9086 - acc: 0.6803 - val_loss: 2.0037 - val_acc: 0.3838\n",
      "Epoch 13/50\n",
      "1054/1054 [==============================] - 103s 97ms/step - loss: 0.8897 - acc: 0.6849 - val_loss: 0.9152 - val_acc: 0.6772\n",
      "Epoch 14/50\n",
      "1054/1054 [==============================] - 100s 95ms/step - loss: 0.8816 - acc: 0.6849 - val_loss: 0.9765 - val_acc: 0.6625\n",
      "Epoch 15/50\n",
      "1054/1054 [==============================] - 113s 107ms/step - loss: 0.8836 - acc: 0.6809 - val_loss: 0.9530 - val_acc: 0.6727\n",
      "Epoch 16/50\n",
      "1054/1054 [==============================] - 101s 95ms/step - loss: 0.8820 - acc: 0.6862 - val_loss: 0.9122 - val_acc: 0.6753\n",
      "Epoch 17/50\n",
      "1054/1054 [==============================] - 101s 96ms/step - loss: 0.8882 - acc: 0.6860 - val_loss: 0.9090 - val_acc: 0.6769\n",
      "Epoch 18/50\n",
      "1054/1054 [==============================] - 100s 95ms/step - loss: 0.8764 - acc: 0.6875 - val_loss: 0.9423 - val_acc: 0.6734\n",
      "Epoch 19/50\n",
      "1054/1054 [==============================] - 105s 100ms/step - loss: 0.8693 - acc: 0.6879 - val_loss: 0.9224 - val_acc: 0.6753\n",
      "Epoch 20/50\n",
      "1054/1054 [==============================] - 108s 103ms/step - loss: 0.8727 - acc: 0.6858 - val_loss: 0.9102 - val_acc: 0.6788\n",
      "Epoch 21/50\n",
      "1054/1054 [==============================] - 99s 94ms/step - loss: 0.8687 - acc: 0.6885 - val_loss: 0.9285 - val_acc: 0.6728\n",
      "Epoch 22/50\n",
      "1054/1054 [==============================] - 101s 96ms/step - loss: 0.8680 - acc: 0.6891 - val_loss: 0.9159 - val_acc: 0.6801\n",
      "Epoch 23/50\n",
      "1053/1054 [============================>.] - ETA: 0s - loss: 0.8731 - acc: 0.6869\n",
      "Epoch 00023: reducing learning rate to 0.0002500000118743628.\n",
      "1054/1054 [==============================] - 103s 98ms/step - loss: 0.8735 - acc: 0.6868 - val_loss: 0.9344 - val_acc: 0.6728\n",
      "Epoch 24/50\n",
      "1054/1054 [==============================] - 101s 96ms/step - loss: 0.8725 - acc: 0.6890 - val_loss: 0.8997 - val_acc: 0.6819\n",
      "Epoch 25/50\n",
      "1054/1054 [==============================] - 99s 94ms/step - loss: 0.8560 - acc: 0.6880 - val_loss: 0.9481 - val_acc: 0.6738\n",
      "Epoch 26/50\n",
      "1054/1054 [==============================] - 102s 97ms/step - loss: 0.8524 - acc: 0.6914 - val_loss: 0.9303 - val_acc: 0.6708\n",
      "Epoch 27/50\n",
      "1054/1054 [==============================] - 102s 96ms/step - loss: 0.8555 - acc: 0.6946 - val_loss: 0.9115 - val_acc: 0.6807\n",
      "Epoch 28/50\n",
      "1054/1054 [==============================] - 103s 98ms/step - loss: 0.8588 - acc: 0.6903 - val_loss: 0.8911 - val_acc: 0.6836\n",
      "Epoch 29/50\n",
      "1054/1054 [==============================] - 103s 98ms/step - loss: 0.8531 - acc: 0.6898 - val_loss: 0.8941 - val_acc: 0.6834\n",
      "Epoch 30/50\n",
      "1054/1054 [==============================] - 100s 95ms/step - loss: 0.8381 - acc: 0.6944 - val_loss: 0.9198 - val_acc: 0.6779\n",
      "Epoch 31/50\n",
      "1054/1054 [==============================] - 102s 97ms/step - loss: 0.8350 - acc: 0.6971 - val_loss: 0.9316 - val_acc: 0.6739\n",
      "Epoch 32/50\n",
      "1054/1054 [==============================] - 102s 97ms/step - loss: 0.8518 - acc: 0.6938 - val_loss: 0.9236 - val_acc: 0.6769\n",
      "Epoch 33/50\n",
      "1054/1054 [==============================] - 102s 97ms/step - loss: 0.8436 - acc: 0.6941 - val_loss: 0.8911 - val_acc: 0.6842\n",
      "Epoch 34/50\n",
      "1053/1054 [============================>.] - ETA: 0s - loss: 0.8506 - acc: 0.6915\n",
      "Epoch 00034: reducing learning rate to 0.0001250000059371814.\n",
      "1054/1054 [==============================] - 101s 96ms/step - loss: 0.8508 - acc: 0.6913 - val_loss: 0.9036 - val_acc: 0.6794\n",
      "Epoch 35/50\n",
      "1054/1054 [==============================] - 109s 104ms/step - loss: 0.8452 - acc: 0.6914 - val_loss: 0.8993 - val_acc: 0.6805\n",
      "Epoch 36/50\n",
      "1054/1054 [==============================] - 102s 96ms/step - loss: 0.8322 - acc: 0.6952 - val_loss: 0.8956 - val_acc: 0.6836\n",
      "Epoch 37/50\n",
      "1054/1054 [==============================] - 103s 98ms/step - loss: 0.8417 - acc: 0.6938 - val_loss: 0.9317 - val_acc: 0.6761\n",
      "Epoch 38/50\n",
      "1054/1054 [==============================] - 100s 95ms/step - loss: 0.8269 - acc: 0.6973 - val_loss: 0.8899 - val_acc: 0.6858\n",
      "Epoch 39/50\n",
      "1054/1054 [==============================] - 102s 96ms/step - loss: 0.8267 - acc: 0.6960 - val_loss: 0.8915 - val_acc: 0.6860\n",
      "Epoch 40/50\n",
      "1054/1054 [==============================] - 108s 103ms/step - loss: 0.8244 - acc: 0.7012 - val_loss: 0.8990 - val_acc: 0.6814\n",
      "Epoch 41/50\n",
      "1054/1054 [==============================] - 104s 99ms/step - loss: 0.8381 - acc: 0.6923 - val_loss: 0.9011 - val_acc: 0.6830\n",
      "Epoch 42/50\n",
      "1054/1054 [==============================] - 101s 96ms/step - loss: 0.8347 - acc: 0.6965 - val_loss: 0.8817 - val_acc: 0.6888\n",
      "Epoch 43/50\n",
      "1054/1054 [==============================] - 100s 95ms/step - loss: 0.8345 - acc: 0.6929 - val_loss: 0.8936 - val_acc: 0.6864\n",
      "Epoch 44/50\n",
      "1054/1054 [==============================] - 103s 98ms/step - loss: 0.8279 - acc: 0.6957 - val_loss: 0.9020 - val_acc: 0.6789\n",
      "Epoch 45/50\n",
      "1054/1054 [==============================] - 102s 97ms/step - loss: 0.8332 - acc: 0.6950 - val_loss: 0.9198 - val_acc: 0.6791\n",
      "Epoch 46/50\n",
      "1054/1054 [==============================] - 102s 97ms/step - loss: 0.8103 - acc: 0.7063 - val_loss: 0.9490 - val_acc: 0.6726\n",
      "Epoch 47/50\n",
      "1054/1054 [==============================] - 101s 96ms/step - loss: 0.8317 - acc: 0.6954 - val_loss: 0.8826 - val_acc: 0.6879\n",
      "Epoch 48/50\n",
      "1052/1054 [============================>.] - ETA: 0s - loss: 0.8226 - acc: 0.6988\n",
      "Epoch 00048: reducing learning rate to 6.25000029685907e-05.\n",
      "1054/1054 [==============================] - 101s 96ms/step - loss: 0.8229 - acc: 0.6986 - val_loss: 0.8920 - val_acc: 0.6831\n",
      "Epoch 49/50\n",
      "1054/1054 [==============================] - 102s 97ms/step - loss: 0.8166 - acc: 0.7032 - val_loss: 0.8866 - val_acc: 0.6868\n",
      "Epoch 50/50\n",
      "1054/1054 [==============================] - 110s 104ms/step - loss: 0.8148 - acc: 0.7003 - val_loss: 0.8891 - val_acc: 0.6857\n",
      "{'acc': 0.7003083491461101, 'val_acc': 0.6856617647058824}\n"
     ]
    }
   ],
   "source": [
    "# Обучаем классификатор.\n",
    "# Сохраняем лог обучения и веса модели.\n",
    "# Выводим accuracy на обучающей и тестовой выборках.\n",
    "\n",
    "import json\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1),\n",
    "    EarlyStopping(monitor='val_loss', patience=25, verbose=1),\n",
    "    ModelCheckpoint('weights/baseline_best', monitor='val_loss', period=5, save_best_only=True),\n",
    "]\n",
    "\n",
    "\n",
    "def process_training_results(model, history, name='main'):\n",
    "    with open(f'logs/{name}.json', 'w') as fout:\n",
    "        json.dump(history.history, fout, sort_keys=True, indent=4)\n",
    "    model.save_weights(f'weights/{name}')\n",
    "    return {'acc': history.history['acc'][-1], 'val_acc': history.history['val_acc'][-1]}\n",
    "\n",
    "\n",
    "history = classifier.fit_generator(gen_train, steps_per_epoch=len(batches_train_x) // 6,\n",
    "                                   epochs=50, callbacks=callbacks,\n",
    "                                   validation_data=gen_test, validation_steps=len(batches_test_x))\n",
    "metrics_baseline = process_training_results(classifier, history, name='baseline')\n",
    "print(metrics_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Освободим ресурсы GPU.\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Baseline [4 балла] Активное обучение\n",
    "\n",
    "Подход активного обучения основан на следующей идее: вместо всего обучающего множества мы используем его маленькие фрагменты, в которых модель неуверена для обучения. Таким образом, модель обучается исключительно по **трудным** объектам, число которых существенно меньше, чем общее число объектов.\n",
    "\n",
    "Обучение модели начинается с обучения по $N$ случайно выбранным примерам, где $N$ – небольшое число (100, 200 и т.д.). Затем модель тестируется на $|train| - N$ объектах, после чего из  $|train| - N$ объектов выбираются снова $N$  объектов, в которых модель не уверена. Эти объекты используются для дообучения модели. Процесс выбора $N$ трудных объектов и дообучения на них повторяется некоторое количество раз (100, 200 и т.д. раз). На каждом шаге активного обучения модель можно протестировать на тестовом множестве, чтобы сравнить ее качества с baseline.\n",
    "\n",
    "Как выбирать трудные объекты:\n",
    "1. Выход нейронной сети - оценки 5 вероятностей принадлежности объекта одному из классов. Предсказанный класс – это тот класс, вероятность которого максимальна. Отсортируем объекты по убыванию вероятности предсказанного класса ($\\min \\max p_i$) и выберем $N$ первых объектов;\n",
    "2. Используем энтропию: чем больше энтропия предсказания, тем ближе распределение вероятностей предсказания к равномерному распределению, тем труднее объект. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsample factor: 16\n"
     ]
    }
   ],
   "source": [
    "# Создаем новый классификатор с той же архитектурой, компилируем его.\n",
    "\n",
    "classifier, downsample_factor = create_classifier_model((None,), CLASS_COUNT, fasttext_model)\n",
    "print(f'Downsample factor: {downsample_factor}')\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Задаем гиперпараметры нашего эксперимента.\n",
    "\n",
    "# Количество \"трудных\" объектов, на которых будет учиться сеть в рамках одной итерации.\n",
    "CHUNK_SIZE = 256\n",
    "# Количество итераций.\n",
    "ITER_COUNT = 100\n",
    "# Количество эпох в рамках одной итерации.\n",
    "CHUNK_EPOCH_COUNT = 4\n",
    "# Частота замера качества модели на тестовой выборке.\n",
    "EVAL_FREQ = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 1\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 2\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 3\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 4\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 5\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 6\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 7\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 8\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 9\n",
      "Training... done\n",
      "Evaluating on test... done\n",
      "\tloss: 2.1596701613696294\n",
      "\tacc: 0.2078688330170778\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 10\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 11\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 12\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 13\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 14\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 15\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 16\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 17\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 18\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 19\n",
      "Training... done\n",
      "Evaluating on test... done\n",
      "\tloss: 1.5606152506888027\n",
      "\tacc: 0.2567599620493359\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 20\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 21\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 22\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 23\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 24\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 25\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 26\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 27\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 28\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 29\n",
      "Training... done\n",
      "Evaluating on test... done\n",
      "\tloss: 2.1044007456336575\n",
      "\tacc: 0.5473493833017078\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 30\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 31\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 32\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 33\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 34\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 35\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 36\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 37\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 38\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 39\n",
      "Training... done\n",
      "Evaluating on test... done\n",
      "\tloss: 1.7421754947112453\n",
      "\tacc: 0.6223612428842504\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 40\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 41\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 42\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 43\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 44\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 45\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 46\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 47\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 48\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 49\n",
      "Training... done\n",
      "Evaluating on test... done\n",
      "\tloss: 2.558560185855435\n",
      "\tacc: 0.5697936432637571\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 50\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 51\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 52\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 53\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 54\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 55\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 56\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 57\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 58\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 59\n",
      "Training... done\n",
      "Evaluating on test... done\n",
      "\tloss: 2.1277397661920516\n",
      "\tacc: 0.46955052182163187\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 60\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 61\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 62\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 63\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 64\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 65\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 66\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 67\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 68\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 69\n",
      "Training... done\n",
      "Evaluating on test... done\n",
      "\tloss: 2.1711540609824365\n",
      "\tacc: 0.6008953984819735\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 70\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 71\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 72\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 73\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 74\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 75\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 76\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 77\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 78\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 79\n",
      "Training... done\n",
      "Evaluating on test... done\n",
      "\tloss: 1.2769695367148073\n",
      "\tacc: 0.3604423624288425\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 80\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 81\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 82\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 83\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 84\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 85\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 86\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 87\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 88\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 89\n",
      "Training... done\n",
      "Evaluating on test... done\n",
      "\tloss: 1.3483952140785485\n",
      "\tacc: 0.5806748102466793\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 90\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 91\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 92\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 93\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 94\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 95\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 96\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 97\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 98\n",
      "Training... done\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n",
      "Iter 99\n",
      "Training... done\n",
      "Evaluating on test... done\n",
      "\tloss: 1.179991923569502\n",
      "\tacc: 0.6143856736242884\n",
      "Updating indices... done\n",
      "Predicting on part of train... done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Реализуем идею активного обучения согласно описанию.\n",
    "\n",
    "used_indices = set()\n",
    "chunk_indices = set(random.sample(list(range(len(tensors_train_sorted))), CHUNK_SIZE))\n",
    "\n",
    "for iter_idx in range(ITER_COUNT):\n",
    "    print(f'Iter {iter_idx}')\n",
    "    print('Training...', end=' ')\n",
    "    # Разбиваем выбранные \"трудные\" объекты на батчи, создаем итератор по ним, обучаем сеть.\n",
    "    chunk_tensors = [tensors_train_sorted[idx] for idx in chunk_indices]\n",
    "    chunk_ratings = [ratings_train_sorted[idx] for idx in chunk_indices]\n",
    "    chunk_batches_tensors, chunk_batches_ratings = make_batches(chunk_tensors, chunk_ratings, BATCH_SIZE)\n",
    "    gen_batches = iterate_over_batches(chunk_batches_tensors, chunk_batches_ratings, randomize=True)\n",
    "    classifier.fit_generator(gen_batches, steps_per_epoch=len(chunk_batches_tensors), epochs=CHUNK_EPOCH_COUNT, verbose=0)\n",
    "    print('done')\n",
    "\n",
    "    if iter_idx % EVAL_FREQ == EVAL_FREQ - 1:\n",
    "        print('Evaluating on test...', end=' ')\n",
    "        # Замеряем качество на тестовой выборке.\n",
    "        metrics = classifier.evaluate_generator(gen_test, steps=len(batches_test_x))\n",
    "        print('done')\n",
    "        for metric_name, metric_value in zip(classifier.metrics_names, metrics):\n",
    "            print(f'\\t{metric_name}: {metric_value}')\n",
    "\n",
    "    print('Updating indices...', end=' ')\n",
    "    # Сгенерируем батчи из той части обучающей выборки, которая еще не участвовала в обучении.\n",
    "    used_indices |= chunk_indices\n",
    "    not_used_indices = [idx for idx in range(len(tensors_train_sorted)) if idx not in used_indices]\n",
    "    not_used_tensors = [tensors_train_sorted[idx] for idx in not_used_indices]\n",
    "    not_used_ratings = [ratings_train_sorted[idx] for idx in not_used_indices]\n",
    "    not_used_batches_tensors, not_used_batches_ratings = make_batches(not_used_tensors, not_used_ratings, BATCH_SIZE)\n",
    "    gen_batches = iterate_over_batches(not_used_batches_tensors, not_used_batches_ratings, randomize=False)\n",
    "    print('done')\n",
    "    \n",
    "    # Предскажем классы на этой части выборки и выберем CHUNK_SIZE новых \"трудных\" объектов.\n",
    "    print('Predicting on part of train...', end=' ')\n",
    "    predictions = classifier.predict_generator(gen_batches, steps=len(not_used_batches_tensors))\n",
    "    predictions_indices = sorted(list(range(len(predictions))), key=lambda idx: predictions[idx].max())\n",
    "    chunk_indices = set(predictions_indices[:CHUNK_SIZE])\n",
    "    print('done')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
