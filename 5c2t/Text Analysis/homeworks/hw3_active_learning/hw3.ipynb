{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 3 [10 баллов] \n",
    "# До 16.05.18 23:59\n",
    "\n",
    "Задание выполняется в группе (1-4 человека). В случае использования какого-либо строннего источника информации обязательно дайте на него ссылку (поскольку другие тоже могут на него наткнуться). Плагиат наказывается нулём баллов за задание и предвзятым отношением в будущем.\n",
    "\n",
    "Не все части обязательны для выполнения, однако вы можете быть дополнительно оштрафованы за небрежное за выполнение одной или двух частей вместо четырех.\n",
    "\n",
    "При возниконовении проблем с выполнением задания обращайтесь с вопросами к преподавателю. Поэтому настоятельно рекомендуется выполнять задание заранее, оставив запас времени на всевозможные технические проблемы. Если вы начали читать условие в последний вечер и не успели из-за проблем с установкой какой-либо библиотеки — это ваши проблемы.\n",
    "\n",
    "\n",
    "Результат выполнения задания — это отчёт в формате html на основе Jupyter Notebook. Нормальный отчёт должен включать в себя:\n",
    "* Краткую постановку задачи и формулировку задания\n",
    "* Описание **минимума** необходимой теории и/или описание используемых инструментов - не стоит переписывать лекции или Википедию\n",
    "* Подробный пошаговый рассказ о проделанной работе\n",
    "* Аккуратно оформленные результаты\n",
    "* **Внятные выводы** – не стоит относится к домашнему заданию как к последовательности сугубо технических шагов, а стоит относится скорее как к небольшому практическому исследованию, у которого есть своя цель и свое назначение.\n",
    "\n",
    "Небрежное его оформление отчета существенно отразится на итоговой оценке. Весь код из отчёта должен быть воспроизводимым, если для этого нужны какие-то дополнительные действия, установленные модули и т.п. — всё это должно быть прописано в тексте в явном виде.\n",
    "\n",
    "Сдача отчетов осуществляется через систему AnyTask.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация текстов с активным обучением\n",
    "\n",
    "\n",
    "Зададимся простой задачей классификации текстов: например, классификацией отзывов на банки по тональности. Эта задача решается с достаточно высокими показателями качества с использованием стандартных алгоритмов классификации, например, сверточных нейронных сетей: корпус состоит из достаточного количества документов, чтобы сверточная сеть хорошо обучилась. Однако возникает естественный вопрос: действительно ли все документы нужны для того, чтобы достичь таких высоких показателей качества (или сопоставимых с ними). Парадигма активного обучения поможет вам ответить на этот вопрос."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Предобработка данных [2 балла]\n",
    "\n",
    "Коллекция отзывов хранится в файле banki_responses (https://www.dropbox.com/s/ol3ux3ibr6rd5ke/banki_responses.json.bz2?dl=0). Одна строчка в этом файле соответствует одному json-словарю. Из этого словаря вам понадобятся два значения по ключам text и rating -- текст отзыва и его оценка по шкале от 1 до 5.   \n",
    "\n",
    "Считайте файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def split_into_words(sentence):\n",
    "    regexp = \"[^а-яА-ЯёЁa-zA-Z]\"\n",
    "    sentence = re.sub(regexp, \" \", sentence)\n",
    "    return sentence.lower().split()\n",
    "\n",
    "\n",
    "def read_data(path='./data/banki_responses.json'):\n",
    "    reviews = []\n",
    "    with open(path, 'r') as fin:\n",
    "        for line in fin:\n",
    "            reviews.append(json.loads(line.strip()))\n",
    "    return [{'text': review['text'],\n",
    "             'words': split_into_words(review['text']),\n",
    "             'rating': review['rating_grade']}\n",
    "            for review in reviews\n",
    "            if review['rating_grade'] is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reviews = read_data()\n",
    "print(f'Количество отзывов: {len(reviews)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитайте, каких отзывов больше: положительных (с оценкой 5) или отрицательных (с оценкой 1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_positive_count = sum(1 for review in reviews if review['rating'] == 5)\n",
    "reviews_negative_count = sum(1 for review in reviews if review['rating'] == 1)\n",
    "print(f'Количество положительных отзывов: {reviews_positive_count}.')\n",
    "print(f'Количество отрицательных отзывов: {reviews_negative_count}.')\n",
    "print(('Положительных' if reviews_negative_count < reviews_positive_count else 'Отрицательных')\n",
    "      + ' отзывов больше.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите предварительную обработку данных: удалите слишком короткие и слишком длинные тексты (пороги на длину определите самостоятельно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценим распределение количества слов в отзывах, построив гистограмму.\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "plt.hist([len(review['words']) for review in reviews], 2000, facecolor='green', alpha=0.7)\n",
    "plt.xlabel('Review length (in words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценим адекватность самых коротких отзывов,\n",
    "# чтобы выявить нижнюю границу на количество слов в отзыве выборки.\n",
    "\n",
    "for review in reviews:\n",
    "    if len(review['words']) <= 5:\n",
    "        print(f'{review[\"rating\"]}: {review[\"text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Из гистограммы видно, что отзывов с количеством слов >= 1000, очень мало.\n",
    "# Поставим соответствующее ограничение сверху на количество слов в отзыве.\n",
    "\n",
    "# Из предыдущего примера можно видеть, что самые короткие отзывы состоят из 4-5 слов,\n",
    "# но при этом являются вполне адекватными, поэтому на текущем этапе причин для их удаления из выборки я не вижу.\n",
    "\n",
    "# Однако, заглядывая вперед, можно увидеть, что сверточной сети придется работать с изображениями большого разрешения:\n",
    "# (review_len, embedding_len),\n",
    "# где review_len -- количество слов в отзыве,\n",
    "# а embedding_len -- размер выходного вектора эмбеддинга.\n",
    "# Посему хорошо бы уменьшить разрешение на входе сетки, а также заметить,\n",
    "# что в ней будут использоваться pooling-слои, также понижающие размерность изображения.\n",
    "# По этой причине исходная длина отзыва (в словах) должна быть не меньше чем downsample factor сети.\n",
    "\n",
    "reviews = [review for review in reviews if 16 <= len(review['words']) < 1000]\n",
    "print(f'Количество отзывов после фильтрации по количеству слов: {len(reviews)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбейте данные на обучающее ($train$) и тестовое ($test$) множество случайным образом в отношеннии 3:1 (или любом другом отношении, которое покажется вам разумным).\n",
    "Задача классификации сформулирована так: по каждому отзыву определить его оценку (т.е. классификация на 5 классов). Признаками для классификации выступают слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "TRAIN_PERCENT = 0.75\n",
    "\n",
    "reviews_indices_train = random.sample(list(range(len(reviews))),\n",
    "                                      int(TRAIN_PERCENT * len(reviews)))\n",
    "reviews_indices_test = list(set(range(len(reviews))) - set(reviews_indices_train))\n",
    "\n",
    "reviews_train = [reviews[idx] for idx in reviews_indices_train]\n",
    "reviews_test = [reviews[idx] for idx in reviews_indices_test]\n",
    "\n",
    "print(f'В обучающей выборке {len(reviews_train)} отзывов.')\n",
    "print(f'В тестовой выборке {len(reviews_test)} отзывов.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Загружаем fasttext модель, ограничиваясь первыми limit словами.\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "fasttext_model = KeyedVectors.load_word2vec_format('../hw2_senna/wiki.ru.vec', limit=999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_tensor_from_review(review, fasttext_model):\n",
    "    words = review['words']\n",
    "    tensor = np.zeros((len(words),))\n",
    "    for idx, word in enumerate(words):\n",
    "        word_in_vocab = fasttext_model.vocab.get(word, None)\n",
    "        if word_in_vocab is not None:\n",
    "            tensor[idx] = word_in_vocab.index\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "CLASS_COUNT = 5\n",
    "\n",
    "tensors_train = [make_tensor_from_review(review, fasttext_model) for review in reviews_train]\n",
    "tensors_test = [make_tensor_from_review(review, fasttext_model) for review in reviews_test]\n",
    "\n",
    "indices_train = sorted(list(range(len(tensors_train))), key=lambda idx: tensors_train[idx].shape[0])\n",
    "indices_test = sorted(list(range(len(tensors_test))), key=lambda idx: tensors_test[idx].shape[0])\n",
    "\n",
    "tensors_train_sorted = [tensors_train[idx] for idx in indices_train]\n",
    "tensors_test_sorted = [tensors_test[idx] for idx in indices_test]\n",
    "ratings_train_sorted = [to_categorical(reviews_train[idx]['rating'] - 1, CLASS_COUNT)\n",
    "                        for idx in indices_train]\n",
    "ratings_test_sorted = [to_categorical(reviews_test[idx]['rating'] - 1, CLASS_COUNT)\n",
    "                       for idx in indices_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "batches_train_x = [tensors_train_sorted[idx:idx + BATCH_SIZE] for idx in range(0, len(tensors_train_sorted), BATCH_SIZE)]\n",
    "batches_train_y = [np.array(ratings_train_sorted[idx:idx + BATCH_SIZE]) for idx in range(0, len(ratings_train_sorted), BATCH_SIZE)]\n",
    "batches_test_x = [tensors_test_sorted[idx:idx + BATCH_SIZE] for idx in range(0, len(tensors_test_sorted), BATCH_SIZE)]\n",
    "batches_test_y = [np.array(ratings_test_sorted[idx:idx + BATCH_SIZE]) for idx in range(0, len(ratings_test_sorted), BATCH_SIZE)]\n",
    "\n",
    "batches_train_x = [pad_sequences(batch, padding='post') for batch in batches_train_x]\n",
    "batches_test_x = [pad_sequences(batch, padding='post') for batch in batches_test_x]\n",
    "\n",
    "print('Количество батчей в обучающей выборке: {len(batches_train_x)}.')\n",
    "print('Количество батчей в тестовой выборке: {len(batches_test_x)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_over_batches(batches_x, batches_y, randomize=False):\n",
    "    indices = list(range(len(batches_x)))\n",
    "    while True:\n",
    "        if randomize:\n",
    "            random.shuffle(indices)\n",
    "        for idx in indices:\n",
    "            yield batches_x[idx], batches_y[idx]\n",
    "\n",
    "\n",
    "gen_train = iterate_over_batches(batches_train_x, batches_train_y, randomize=True)\n",
    "gen_test = iterate_over_batches(batches_test_x, batches_test_y, randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 0\n",
    "# print(reviews_train[indices_train[idx]])\n",
    "# print(tensors_train_sorted[idx])\n",
    "# print(ratings_train_sorted[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Baseline [4 балла]\n",
    "\n",
    "Получите baseline классификации: в идеале, используйте сверточную нейронную сеть (слой эмбеддингов + свертка + субдескритизация). Число и размерность фильтров определите самостоятельно, так же как и использование регуляризаторов (dropout / batch norm) и их параметров. Так же самостоятельно (но обосновано) решите, использовать ли вам предобученные эмбеддинги или нет и проводить ли вам лемматизацию или нет. \n",
    "\n",
    "Обучите сеть на обучающем множестве и протестируйте на тестовом. Зафиксируйте baseline.\n",
    "\n",
    "Если совсем трудно или вычисления занимают слишком много времени, используйте любой другой известный и симпатичный вам алгоритм классификации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, GlobalAveragePooling2D, MaxPool2D, Dropout\n",
    "from keras.layers import Reshape, BatchNormalization, LeakyReLU, Activation\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "class DarknetBlock:\n",
    "    def __init__(self, filters1, filters3, strides=(1, 1)):\n",
    "        self._filters1 = filters1\n",
    "        self._filters3 = filters3\n",
    "        self._strides = strides\n",
    "\n",
    "    def __call__(self, input_tensor, *args, **kwargs):\n",
    "        x = input_tensor\n",
    "        for filters, size, strides in [(self._filters3, (3, 3), self._strides),\n",
    "                                       (self._filters1, (1, 1), (1, 1))]:\n",
    "            x = Conv2D(filters, size, strides=strides, padding='same',\n",
    "                       kernel_initializer='he_normal', kernel_regularizer=l2(1e-10))(x)\n",
    "            x = LeakyReLU(alpha=0.05)(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def create_classifier_model(input_shape, class_count, fasttext_model, filter_counts=None):\n",
    "    filter_counts = filter_counts or [16, 32, 64]\n",
    "    downsample_factor = 1\n",
    "\n",
    "    # Пользуемся функционалом gensim.KeyedVectors для создания слоя Embedding.\n",
    "    # и инициализации его весами.\n",
    "    embedding_layer = fasttext_model.get_keras_embedding()\n",
    "    # Также отмечаем, что эти веса не надо менять при обучении сети.\n",
    "    embedding_layer.trainable = False\n",
    "\n",
    "    input_layer = Input(input_shape)\n",
    "    embedding = embedding_layer(input_layer)\n",
    "    reshaped = Reshape((1, -1, fasttext_model.vector_size))(embedding)\n",
    "    x = reshaped\n",
    "    x = Conv2D(10, (3, 3), strides=(2, 2), padding='same',\n",
    "               kernel_initializer='he_normal', kernel_regularizer=l2(1e-10))(x)\n",
    "    downsample_factor *= 2\n",
    "    x = Conv2D(20, (3, 3), strides=(2, 2), padding='same',\n",
    "               kernel_initializer='he_normal', kernel_regularizer=l2(1e-10))(x)\n",
    "    downsample_factor *= 2\n",
    "    for idx, filter_count in enumerate(filter_counts):\n",
    "        x = DarknetBlock(filter_count, filter_count * 2)(x)\n",
    "        x = DarknetBlock(filter_count, filter_count * 2)(x)\n",
    "        if idx != len(filter_counts) - 1:\n",
    "            x = MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
    "            downsample_factor *= 2\n",
    "            x = BatchNormalization(axis=1)(x)\n",
    "            x = Dropout(0.1)(x)\n",
    "    x = Conv2D(class_count, (1, 1), padding='same')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    output = Activation('softmax')(x)\n",
    "    return Model(inputs=[input_layer], outputs=[output]), downsample_factor\n",
    "\n",
    "\n",
    "classifier, downsample_factor = create_classifier_model((None,), CLASS_COUNT, fasttext_model)\n",
    "print(f'Downsample factor: {downsample_factor}')\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем классификатор.\n",
    "# Сохраняем лог обучения и веса модели.\n",
    "\n",
    "import json\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1),\n",
    "    EarlyStopping(monitor='val_loss', patience=25, verbose=1),\n",
    "    ModelCheckpoint('weights/baseline_best', monitor='val_loss', period=5, save_best_only=True),\n",
    "]\n",
    "\n",
    "\n",
    "def process_training_results(model, history, name='main'):\n",
    "    with open(f'logs/{name}.json', 'w') as fout:\n",
    "        json.dump(history.history, fout, sort_keys=True, indent=4)\n",
    "    model.save_weights(f'weights/{name}')\n",
    "    return {'acc': history.history['acc'][-1], 'val_acc': history.history['val_acc'][-1]}\n",
    "\n",
    "\n",
    "history = classifier.fit_generator(gen_train, steps_per_epoch=len(batches_train_x) // 12,\n",
    "                                   epochs=100, callbacks=callbacks,\n",
    "                                   validation_data=gen_test, validation_steps=len(batches_test_x))\n",
    "metrics = process_training_results(classifier, history, name='baseline')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Baseline [4 балла] Активное обучение\n",
    "\n",
    "Подход активного обучения основан на следующей идее: вместо всего обучающего множества мы используем его маленькие фрагменты, в которых модель неуверена для обучения. Таким образом, модель обучается исключительно по **трудным** объектам, число которых существенно меньше, чем общее число объектов.\n",
    "\n",
    "Обучение модели начинается с обучения по $N$ случайно выбранным примерам, где $N$ – небольшое число (100, 200 и т.д.). Затем модель тестируется на $|train| - N$ объектах, после чего из  $|train| - N$ объектов выбираются снова $N$  объектов, в которых модель не уверена. Эти объекты используются для дообучения модели. Процесс выбора $N$ трудных объектов и дообучения на них повторяется некоторое количество раз (100, 200 и т.д. раз). На каждом шаге активного обучения модель можно протестировать на тестовом множестве, чтобы сравнить ее качества с baseline.\n",
    "\n",
    "Как выбирать трудные объекты:\n",
    "1. Выход нейронной сети - оценки 5 вероятностей принадлежности объекта одному из классов. Предсказанный класс – это тот класс, вероятность которого максимальна. Отсортируем объекты по убыванию вероятности предсказанного класса ($\\min \\max p_i$) и выберем $N$ первых объектов;\n",
    "2. Используем энтропию: чем больше энтропия предсказания, тем ближе распределение вероятностей предсказания к равномерному распределению, тем труднее объект. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
