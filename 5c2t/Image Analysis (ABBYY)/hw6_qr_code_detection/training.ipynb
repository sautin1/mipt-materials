{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import chain\n",
    "import random\n",
    "from os.path import join\n",
    "\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "from data import DatasetReader\n",
    "from data import PATH_ABBYY_DATASET, PATH_BARCODES_DATASET, PATH_DUBSKA_MATRIX_DATASET\n",
    "from data import PATH_DUBSKA_QRCODE_DATASET, PATH_SOEROES_DATASET\n",
    "from models import DarknetModel\n",
    "\n",
    "from utils.io import read_image\n",
    "from utils.paths import PATH_PROJECT\n",
    "from utils.image import resize_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_artefacts = PATH_PROJECT\n",
    "path_weights = join(path_artefacts, 'weights_detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "IMAGE_SHAPE = (3, None, None)\n",
    "EPOCHS = 30\n",
    "TRAIN_PERCENT = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsample factor: 8\n"
     ]
    }
   ],
   "source": [
    "K.set_image_data_format('channels_first')\n",
    "feature_extractor = DarknetModel(IMAGE_SHAPE, use_dropout=True, filter_counts=[60, 100, 140])\n",
    "DOWNSAMPLE_FACTOR = feature_extractor.get_downsample_factor()\n",
    "print(f'Downsample factor: {DOWNSAMPLE_FACTOR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_ground_truth_image(shape, markup):\n",
    "    image = np.zeros(shape)\n",
    "    for corner in markup:\n",
    "        points = np.array(corner['pts']) // DOWNSAMPLE_FACTOR\n",
    "        for point in points:\n",
    "            if shape[1] <= point[0] or shape[2] <= point[1]:\n",
    "                raise ValueError('Point exceeds image bounds')\n",
    "            image[0, point[0], point[1]] = 1\n",
    "    return image\n",
    "\n",
    "\n",
    "def generate_data(paths, datasets, randomize=True, shrink_size=256):\n",
    "    while True:\n",
    "        indices = list(range(len(paths)))\n",
    "        if randomize:\n",
    "            random.shuffle(indices)\n",
    "        for path_idx in indices:\n",
    "            path, dataset_idx = paths[path_idx]\n",
    "            dataset = datasets[dataset_idx]\n",
    "            image_id = dataset.get_image_id(path)\n",
    "            markup = dataset.get_markup()[image_id]\n",
    "            image = read_image(path)\n",
    "            shrink_factor = max(image.shape[1] // shrink_size, image.shape[2] // shrink_size)\n",
    "            if shrink_factor > 1:\n",
    "                shrink_factor = 1.0 / shrink_factor\n",
    "                image = resize_image(image, fx=shrink_factor, fy=shrink_factor)\n",
    "            try:\n",
    "                ground_truth = create_ground_truth_image((1,\n",
    "                                                          image.shape[1] // DOWNSAMPLE_FACTOR,\n",
    "                                                          image.shape[2] // DOWNSAMPLE_FACTOR), markup)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            yield np.array([image]), np.array([ground_truth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = [DatasetReader(path) for path in [PATH_SOEROES_DATASET,\n",
    "                                             PATH_DUBSKA_QRCODE_DATASET,\n",
    "                                             PATH_DUBSKA_MATRIX_DATASET,\n",
    "                                             PATH_BARCODES_DATASET,\n",
    "                                             PATH_ABBYY_DATASET]]\n",
    "paths = [[(path, idx) for path in dataset.get_paths()]\n",
    "         for idx, dataset in enumerate(datasets)]\n",
    "paths = list(chain(*paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paths_train = random.sample(paths, int(TRAIN_PERCENT * len(paths)))\n",
    "paths_test = list(set(paths) - set(paths_train))\n",
    "\n",
    "batches_train = generate_data(paths_train, datasets)\n",
    "batches_test = generate_data(paths_test, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from utils.visualization import show_image\n",
    "# \n",
    "# gen = batches_train\n",
    "# image, gt = next(gen)\n",
    "# print(image.shape)\n",
    "# print(gt.shape)\n",
    "# \n",
    "# show_image(image[0])\n",
    "# show_image(gt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3, None, None)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 10, None, None)    280       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, None, None)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 120, None, None)   10920     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 120, None, None)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 60, None, None)    7260      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 60, None, None)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 120, None, None)   64920     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 120, None, None)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 60, None, None)    7260      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 60, None, None)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 60, None, None)    0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 60, None, None)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 200, None, None)   108200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 200, None, None)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 100, None, None)   20100     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 100, None, None)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 200, None, None)   180200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 200, None, None)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 100, None, None)   20100     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 100, None, None)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 100, None, None)   0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, None, None)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 280, None, None)   252280    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 280, None, None)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 140, None, None)   39340     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 140, None, None)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 280, None, None)   353080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 280, None, None)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 140, None, None)   39340     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 140, None, None)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 1, None, None)     1261      \n",
      "=================================================================\n",
      "Total params: 1,104,541\n",
      "Trainable params: 1,104,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = feature_extractor.get_model()\n",
    "x = feature_extractor.output\n",
    "x = Conv2D(1, (3, 3), padding='same', activation='sigmoid')(x)\n",
    "detector = Model(inputs=[feature_extractor.input], outputs=[x])\n",
    "detector.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "346/346 [==============================] - 29s 85ms/step - loss: 0.0437 - val_loss: 0.0128\n",
      "Epoch 2/30\n",
      "346/346 [==============================] - 29s 84ms/step - loss: 0.0132 - val_loss: 0.0206\n",
      "Epoch 3/30\n",
      "346/346 [==============================] - 24s 70ms/step - loss: 0.0159 - val_loss: 0.0100\n",
      "Epoch 4/30\n",
      "346/346 [==============================] - 22s 62ms/step - loss: 0.0114 - val_loss: 0.0120\n",
      "Epoch 5/30\n",
      "346/346 [==============================] - 30s 86ms/step - loss: 0.0081 - val_loss: 0.0106\n",
      "Epoch 6/30\n",
      "346/346 [==============================] - 22s 64ms/step - loss: 0.0086 - val_loss: 0.0052\n",
      "Epoch 7/30\n",
      "346/346 [==============================] - 26s 76ms/step - loss: 0.0067 - val_loss: 0.0086\n",
      "Epoch 8/30\n",
      "346/346 [==============================] - 22s 65ms/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 9/30\n",
      "346/346 [==============================] - 35s 101ms/step - loss: 0.0078 - val_loss: 0.0063\n",
      "Epoch 10/30\n",
      "346/346 [==============================] - 23s 65ms/step - loss: 0.0042 - val_loss: 0.0052\n",
      "Epoch 11/30\n",
      "346/346 [==============================] - 21s 61ms/step - loss: 0.0069 - val_loss: 0.0174\n",
      "Epoch 12/30\n",
      "346/346 [==============================] - 29s 83ms/step - loss: 0.0074 - val_loss: 0.0049\n",
      "Epoch 13/30\n",
      "346/346 [==============================] - 30s 86ms/step - loss: 0.0151 - val_loss: 0.0079\n",
      "Epoch 14/30\n",
      "346/346 [==============================] - 23s 67ms/step - loss: 0.0050 - val_loss: 0.0048\n",
      "Epoch 15/30\n",
      "346/346 [==============================] - 24s 68ms/step - loss: 0.0034 - val_loss: 0.0061\n",
      "Epoch 16/30\n",
      "346/346 [==============================] - 28s 82ms/step - loss: 0.0030 - val_loss: 0.0068\n",
      "Epoch 17/30\n",
      "346/346 [==============================] - 28s 81ms/step - loss: 0.0040 - val_loss: 0.0016\n",
      "Epoch 18/30\n",
      "346/346 [==============================] - 27s 78ms/step - loss: 0.0040 - val_loss: 0.0069\n",
      "Epoch 19/30\n",
      "346/346 [==============================] - 21s 62ms/step - loss: 0.0016 - val_loss: 0.0291\n",
      "Epoch 20/30\n",
      "346/346 [==============================] - 28s 81ms/step - loss: 0.0060 - val_loss: 0.0033\n",
      "Epoch 21/30\n",
      "346/346 [==============================] - 22s 63ms/step - loss: 0.0031 - val_loss: 0.0019\n",
      "Epoch 22/30\n",
      "346/346 [==============================] - 29s 85ms/step - loss: 0.0054 - val_loss: 0.0085\n",
      "Epoch 23/30\n",
      "343/346 [============================>.] - ETA: 0s - loss: 0.0052\n",
      "Epoch 00023: reducing learning rate to 0.0005000000237487257.\n",
      "346/346 [==============================] - 21s 61ms/step - loss: 0.0053 - val_loss: 0.0037\n",
      "Epoch 24/30\n",
      "346/346 [==============================] - 18s 53ms/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 25/30\n",
      "346/346 [==============================] - 34s 98ms/step - loss: 0.0012 - val_loss: 0.0034\n",
      "Epoch 26/30\n",
      "346/346 [==============================] - 27s 79ms/step - loss: 9.7227e-04 - val_loss: 0.0017\n",
      "Epoch 27/30\n",
      "346/346 [==============================] - 25s 73ms/step - loss: 4.1711e-04 - val_loss: 0.0018\n",
      "Epoch 28/30\n",
      "346/346 [==============================] - 24s 69ms/step - loss: 5.6239e-04 - val_loss: 0.0012\n",
      "Epoch 29/30\n",
      "346/346 [==============================] - 29s 83ms/step - loss: 0.0013 - val_loss: 0.0097\n",
      "Epoch 30/30\n",
      "346/346 [==============================] - 25s 71ms/step - loss: 0.0025 - val_loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "if args.train:\n",
    "    from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "    \n",
    "    if args.train:\n",
    "        callbacks = [\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1),\n",
    "            ModelCheckpoint(path_weights, monitor='val_loss', period=5, save_best_only=True)\n",
    "            #EarlyStopping(monitor='val_loss', patience=25, verbose=1)\n",
    "        ]\n",
    "        detector.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "        history = detector.fit_generator(batches_train, steps_per_epoch=len(paths_train) // BATCH_SIZE // 4,\n",
    "                                         validation_data=batches_test, validation_steps=len(paths_test) // BATCH_SIZE,\n",
    "                                         epochs=EPOCHS, callbacks=callbacks)\n",
    "        detector.save_weights(path_weights)\n",
    "detector.load_weights(path_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
