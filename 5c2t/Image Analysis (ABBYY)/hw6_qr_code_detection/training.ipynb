{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import chain\n",
    "import random\n",
    "from os.path import join\n",
    "\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "from data import DatasetReader\n",
    "from data import PATH_ABBYY_DATASET, PATH_BARCODES_DATASET, PATH_DUBSKA_MATRIX_DATASET\n",
    "from data import PATH_DUBSKA_QRCODE_DATASET, PATH_SOEROES_DATASET\n",
    "from models import DarknetModel\n",
    "\n",
    "from utils.io import read_image\n",
    "from utils.paths import PATH_PROJECT\n",
    "from utils.image import resize_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_artefacts = PATH_PROJECT\n",
    "path_weights = join(path_artefacts, 'weights_detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "IMAGE_SHAPE = (3, None, None)\n",
    "EPOCHS = 100\n",
    "TRAIN_PERCENT = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_first')\n",
    "feature_extractor = DarknetModel(IMAGE_SHAPE, use_dropout=True, filter_counts=[60, 100, 140])\n",
    "DOWNSAMPLE_FACTOR = feature_extractor.get_downsample_factor()\n",
    "print(f'Downsample factor: {DOWNSAMPLE_FACTOR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_ground_truth_image(shape, markup):\n",
    "    image = np.zeros(shape)\n",
    "    for corner in markup:\n",
    "        points = np.array(corner['pts']) // DOWNSAMPLE_FACTOR\n",
    "        for point in points:\n",
    "            if shape[1] <= point[0] or shape[2] <= point[1]:\n",
    "                raise ValueError('Point exceeds image bounds')\n",
    "            image[0, point[0], point[1]] = 1\n",
    "    return image\n",
    "\n",
    "\n",
    "def generate_data(paths, datasets, randomize=True, shrink_size=256):\n",
    "    while True:\n",
    "        indices = list(range(len(paths)))\n",
    "        if randomize:\n",
    "            random.shuffle(indices)\n",
    "        for path_idx in indices:\n",
    "            path, dataset_idx = paths[path_idx]\n",
    "            dataset = datasets[dataset_idx]\n",
    "            image_id = dataset.get_image_id(path)\n",
    "            markup = dataset.get_markup()[image_id]\n",
    "            image = read_image(path)\n",
    "            shrink_factor = max(image.shape[1] // shrink_size, image.shape[2] // shrink_size)\n",
    "            if shrink_factor > 1:\n",
    "                shrink_factor = 1.0 / shrink_factor\n",
    "                image = resize_image(image, fx=shrink_factor, fy=shrink_factor)\n",
    "            try:\n",
    "                ground_truth = create_ground_truth_image((1,\n",
    "                                                          image.shape[1] // DOWNSAMPLE_FACTOR,\n",
    "                                                          image.shape[2] // DOWNSAMPLE_FACTOR), markup)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            yield np.array([image]), np.array([ground_truth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = [DatasetReader(path) for path in [PATH_SOEROES_DATASET,\n",
    "                                             PATH_DUBSKA_QRCODE_DATASET,\n",
    "                                             PATH_DUBSKA_MATRIX_DATASET,\n",
    "                                             PATH_BARCODES_DATASET,\n",
    "                                             PATH_ABBYY_DATASET]]\n",
    "paths = [[(path, idx) for path in dataset.get_paths()]\n",
    "         for idx, dataset in enumerate(datasets)]\n",
    "paths = list(chain(*paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paths_train = random.sample(paths, int(TRAIN_PERCENT * len(paths)))\n",
    "paths_test = list(set(paths) - set(paths_train))\n",
    "\n",
    "batches_train = generate_data(paths_train, datasets)\n",
    "batches_test = generate_data(paths_test, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from utils.visualization import show_image\n",
    "# \n",
    "# gen = batches_train\n",
    "# image, gt = next(gen)\n",
    "# print(image.shape)\n",
    "# print(gt.shape)\n",
    "# \n",
    "# show_image(image[0])\n",
    "# show_image(gt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = feature_extractor.get_model()\n",
    "x = feature_extractor.output\n",
    "x = Conv2D(1, (3, 3), padding='same', activation='sigmoid')(x)\n",
    "detector = Model(inputs=[feature_extractor.input], outputs=[x])\n",
    "detector.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.train:\n",
    "    from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "    \n",
    "    if args.train:\n",
    "        callbacks = [\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1),\n",
    "            ModelCheckpoint(path_weights, monitor='val_loss', period=5, save_best_only=True),\n",
    "            EarlyStopping(monitor='val_loss', patience=25, verbose=1)\n",
    "        ]\n",
    "        detector.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "        history = detector.fit_generator(batches_train, steps_per_epoch=len(paths_train) // BATCH_SIZE // 4,\n",
    "                                         validation_data=batches_test, validation_steps=len(paths_test) // BATCH_SIZE,\n",
    "                                         epochs=EPOCHS, callbacks=callbacks)\n",
    "        detector.save_weights(path_weights)\n",
    "detector.load_weights(path_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
