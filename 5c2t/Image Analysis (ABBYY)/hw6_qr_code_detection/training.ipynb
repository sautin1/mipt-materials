{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import chain\n",
    "import random\n",
    "from os.path import join\n",
    "\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "from data import DatasetReader\n",
    "from data import PATH_ABBYY_DATASET, PATH_BARCODES_DATASET, PATH_DUBSKA_MATRIX_DATASET\n",
    "from data import PATH_DUBSKA_QRCODE_DATASET, PATH_SOEROES_DATASET\n",
    "from models import DarknetModel\n",
    "\n",
    "from utils.io import read_image\n",
    "from utils.paths import PATH_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "    \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_artefacts = PATH_PROJECT\n",
    "path_weights = join(path_artefacts, 'weights_detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "IMAGE_SHAPE = (3, None, None)\n",
    "STEPS_PER_EPOCH = 1541\n",
    "EPOCHS = 100\n",
    "\n",
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ground_truth_image(shape, markup):\n",
    "    image = np.zeros(shape)\n",
    "    for corner in markup:\n",
    "        for point in corner['pts']:\n",
    "            if shape[1] <= point[0] or shape[2] <= point[1]:\n",
    "                raise ValueError('Point exceeds image bounds')\n",
    "            image[0, point[0], point[1]] = 1\n",
    "    return image\n",
    "\n",
    "\n",
    "def generate_data(randomize=True):\n",
    "    datasets = [DatasetReader(path) for path in [PATH_SOEROES_DATASET,\n",
    "                                                 PATH_DUBSKA_QRCODE_DATASET,\n",
    "                                                 PATH_DUBSKA_MATRIX_DATASET,\n",
    "                                                 PATH_BARCODES_DATASET,\n",
    "                                                 PATH_ABBYY_DATASET]]\n",
    "    paths = [[(path, idx) for path in dataset.get_paths()]\n",
    "             for idx, dataset in enumerate(datasets)]\n",
    "    paths = list(chain(*paths))\n",
    "    while True:\n",
    "        indices = list(range(len(paths)))\n",
    "        if randomize:\n",
    "            random.shuffle(indices)\n",
    "        for path_idx in indices:\n",
    "            path, dataset_idx = paths[path_idx]\n",
    "            dataset = datasets[dataset_idx]\n",
    "            image_id = dataset.get_image_id(path)\n",
    "            markup = dataset.get_markup()[image_id]\n",
    "            image = read_image(path)\n",
    "            try:\n",
    "                ground_truth = create_ground_truth_image((1, *image.shape[1:]), markup)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            yield np.array([image]), np.array([ground_truth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.visualization import show_image\n",
    "\n",
    "# gen = generate_data()\n",
    "# image, gt = next(gen)\n",
    "# print(image.shape)\n",
    "# print(gt.shape)\n",
    "\n",
    "# show_image(image)\n",
    "# show_image(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = DarknetModel(IMAGE_SHAPE, use_dropout=True, filter_counts=[60, 100, 140, 180]).get_model()\n",
    "x = feature_extractor.output\n",
    "x = Conv2D(1, (3, 3), padding='same', activation='sigmoid')(x)\n",
    "detector = Model(inputs=[feature_extractor.input], outputs=[x])\n",
    "detector.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.train:\n",
    "    from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "    \n",
    "    if args.train:\n",
    "        callbacks = [\n",
    "            ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=1),\n",
    "            ModelCheckpoint(path_weights, monitor='loss', period=5, save_best_only=True),\n",
    "            EarlyStopping(monitor='loss', patience=25, verbose=1)\n",
    "        ]\n",
    "        detector.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "        history = detector.fit_generator(generate_data(), steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                                         epochs=EPOCHS, callbacks=callbacks)\n",
    "        detector.save_weights(path_weights)\n",
    "detector.load_weights(path_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
