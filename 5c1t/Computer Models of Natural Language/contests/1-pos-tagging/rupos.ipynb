{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution is based on the notebook by IlyaGusev:\n",
    "https://github.com/IlyaGusev/nlp-practice/blob/master/rupos.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Имена файлов с данными.\n",
    "from os.path import join\n",
    "PATH_DATA = 'data'\n",
    "TRAIN_FILENAME = join(PATH_DATA, 'train.csv')\n",
    "TEST_FILENAME = join(PATH_DATA, 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Считывание файлов.\n",
    "from collections import namedtuple\n",
    "WordForm = namedtuple('WordForm', 'word pos gram')\n",
    "\n",
    "def get_sentences(filename, is_train):\n",
    "    with open(filename, 'r', encoding='utf-8') as r:\n",
    "        sentence = []\n",
    "        for line in r:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                if len(sentence) > 0:\n",
    "                    yield sentence\n",
    "                    sentence = []\n",
    "                continue\n",
    "            if is_train:\n",
    "                line_parts = line.split('\\t')\n",
    "                word = line_parts[2]\n",
    "                pos, gram = line_parts[3].split('#')\n",
    "                sentence.append(WordForm(word, pos, gram))\n",
    "            else:\n",
    "                word = line.split('\\t')[2]\n",
    "                sentence.append(word)\n",
    "        if len(sentence) != 0:\n",
    "            yield sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = list(get_sentences(TRAIN_FILENAME, True))\n",
    "test = list(get_sentences(TEST_FILENAME, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Класс для удобной векторизации грамматических значений.\n",
    "import jsonpickle\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "def process_gram_tag(gram: str):\n",
    "    gram = gram.strip().split('|')\n",
    "    return '|'.join(sorted(gram))\n",
    "\n",
    "\n",
    "def get_empty_category():\n",
    "    return {GrammemeVectorizer.UNKNOWN_VALUE}\n",
    "\n",
    "\n",
    "class GrammemeVectorizer(object):\n",
    "    UNKNOWN_VALUE = 'Unknown'\n",
    "\n",
    "    def __init__(self, dump_filename: str):\n",
    "        self.all_grammemes = defaultdict(get_empty_category)  # type: Dict[str, Set]\n",
    "        self.vectors = []  # type: List[List[int]]\n",
    "        self.name_to_index = {}  # type: Dict[str, int]\n",
    "        self.dump_filename = dump_filename  # type: str\n",
    "        if os.path.exists(self.dump_filename):\n",
    "            self.load()\n",
    "\n",
    "    def add_grammemes(self, pos_tag: str, gram: str) -> int:\n",
    "        gram = process_gram_tag(gram)\n",
    "        vector_name = pos_tag + '#' + gram\n",
    "        if vector_name not in self.name_to_index:\n",
    "            self.name_to_index[vector_name] = len(self.name_to_index)\n",
    "            self.all_grammemes['POS'].add(pos_tag)\n",
    "            gram = gram.split('|') if gram != '_' else []\n",
    "            for grammeme in gram:\n",
    "                category, value = grammeme.split('=')\n",
    "                self.all_grammemes[category].add(value)\n",
    "        return self.name_to_index[vector_name]\n",
    "\n",
    "    def init_possible_vectors(self) -> None:\n",
    "        self.vectors = []\n",
    "        for grammar_val, index in sorted(self.name_to_index.items(), key=lambda x: x[1]):\n",
    "            pos_tag, grammemes = grammar_val.split('#')\n",
    "            grammemes = grammemes.split('|') if grammemes != '_' else []\n",
    "            vector = self.__build_vector(pos_tag, grammemes)\n",
    "            self.vectors.append(vector)\n",
    "\n",
    "    def get_vector(self, vector_name: str) -> List[int]:\n",
    "        if vector_name not in self.name_to_index:\n",
    "            return [0] * len(self.vectors[0])\n",
    "        return self.vectors[self.name_to_index[vector_name]]\n",
    "\n",
    "    def get_vector_by_index(self, index: int) -> List[int]:\n",
    "        return self.vectors[index] if 0 <= index < len(self.vectors) else [0] * len(self.vectors[0])\n",
    "\n",
    "    def get_ordered_grammemes(self) -> List[str]:\n",
    "        flat = []\n",
    "        sorted_grammemes = sorted(self.all_grammemes.items(), key=lambda x: x[0])\n",
    "        for category, values in sorted_grammemes:\n",
    "            for value in sorted(list(values)):\n",
    "                flat.append(category + '=' + value)\n",
    "        return flat\n",
    "    \n",
    "    def save(self) -> None:\n",
    "        with open(self.dump_filename, 'w') as f:\n",
    "            f.write(jsonpickle.encode(self, f))\n",
    "\n",
    "    def load(self):\n",
    "        with open(self.dump_filename, 'r') as f:\n",
    "            vectorizer = jsonpickle.decode(f.read())\n",
    "            self.__dict__.update(vectorizer.__dict__)\n",
    "\n",
    "    def size(self) -> int:\n",
    "        return len(self.vectors)\n",
    "\n",
    "    def grammemes_count(self) -> int:\n",
    "        return len(self.get_ordered_grammemes())\n",
    "\n",
    "    def is_empty(self) -> int:\n",
    "        return len(self.vectors) == 0\n",
    "\n",
    "    def get_name_by_index(self, index):\n",
    "        d = {index: name for name, index in self.name_to_index.items()}\n",
    "        return d[index]\n",
    "\n",
    "    def get_index_by_name(self, name):\n",
    "        pos = name.split('#')[0]\n",
    "        gram = process_gram_tag(name.split('#')[1])\n",
    "        return self.name_to_index[pos + '#' + gram]\n",
    "\n",
    "    def __build_vector(self, pos_tag: str, grammemes: List[str]) -> List[int]:\n",
    "        vector = []\n",
    "        gram_tags = {pair.split('=')[0]: pair.split('=')[1] for pair in grammemes}\n",
    "        gram_tags['POS'] = pos_tag\n",
    "        sorted_grammemes = sorted(self.all_grammemes.items(), key=lambda x: x[0])\n",
    "        for category, values in sorted_grammemes:\n",
    "            value_correct = gram_tags[category] if category in gram_tags else GrammemeVectorizer.UNKNOWN_VALUE\n",
    "            vector.extend(1 if value == value_correct else 0 for value in sorted(list(values)))\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from russian_tagsets import converters\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "to_ud = converters.converter('opencorpora-int', 'ud14')\n",
    "\n",
    "def convert_from_opencorpora_tag(tag, text):\n",
    "    ud_tag = to_ud(str(tag), text)\n",
    "    pos, gram = ud_tag.split()\n",
    "    return pos, gram\n",
    "\n",
    "def fill_all_variants(word, vectorizer):\n",
    "    for parse in morph.parse(word):\n",
    "        pos, gram = convert_from_opencorpora_tag(parse.tag, parse.word)\n",
    "        vectorizer.add_grammemes(pos, gram)\n",
    "\n",
    "vectorizer = GrammemeVectorizer('vectorizer.json')\n",
    "if vectorizer.is_empty():\n",
    "    print('Add train sentences to vectorizer')\n",
    "    for sentence in tqdm(train):\n",
    "        for form in sentence:\n",
    "            fill_all_variants(form.word, vectorizer)\n",
    "    print('Add test sentences to vectorizer')\n",
    "    for sentence in tqdm(test):\n",
    "        for word in sentence:\n",
    "            fill_all_variants(word, vectorizer)\n",
    "    print('Init vectors in vectorizer')\n",
    "    vectorizer.init_possible_vectors()\n",
    "    vectorizer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_output = GrammemeVectorizer('vectorizer_output.json')\n",
    "if vectorizer_output.is_empty():\n",
    "    for sentence in tqdm(train):\n",
    "        for form in sentence:\n",
    "            vectorizer_output.add_grammemes(form.pos, gram)\n",
    "    vectorizer_output.init_possible_vectors()\n",
    "    vectorizer_output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Получение признаков для конкретного контекста.\n",
    "def get_context_features(i, parse_sentence, context_len):\n",
    "    sample = []\n",
    "    left = i - (context_len - 1) // 2\n",
    "    right = i + context_len // 2\n",
    "    if left < 0:\n",
    "        sample.extend(0 for _ in range(vectorizer.grammemes_count() * (-left)))\n",
    "    for parse in parse_sentence[max(left, 0): min(right + 1, len(sentence))]:\n",
    "        pos, gram = convert_from_opencorpora_tag(parse.tag, parse.word)\n",
    "        gram = process_gram_tag(gram)\n",
    "        sample.extend(vectorizer.get_vector(pos + '#' + gram))\n",
    "    if right > len(sentence) - 1:\n",
    "        sample.extend(0 for _ in range(vectorizer.grammemes_count() * (right - len(sentence) + 1)))\n",
    "    assert len(sample) == context_len * vectorizer.grammemes_count()\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False False  True False False\n",
      " False False False False  True False False  True False False False  True\n",
      " False False  True False  True False False False  True False False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False  True False False False  True False  True False False\n",
      " False False  True False False  True False False  True False False  True\n",
      " False False False False False False  True False False  True False False\n",
      " False  True False False  True False  True False False False  True False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False  True False False False  True False  True\n",
      " False False False False  True False False  True False False  True False\n",
      " False  True False False False False False False  True False False  True\n",
      " False False False  True False False  True False  True False False False\n",
      "  True False  True False False False False False False False False False\n",
      " False False False False False False False  True False False False  True\n",
      " False  True False False False False  True False False  True]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Загрузка обучающей выборки.\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "context_len = 5\n",
    "\n",
    "TRAIN_SAMPLES_PATH = 'samples.npy'\n",
    "ANSWERS_PATH = 'answers.npy'\n",
    "if not os.path.exists(TRAIN_SAMPLES_PATH) or not os.path.exists(ANSWERS_PATH):\n",
    "    n = sum([1 for sentence in train for word in sentence])\n",
    "    samples = np.zeros((n, context_len * vectorizer.grammemes_count()), dtype='bool_')\n",
    "    answers = np.zeros((n, ), dtype='int')\n",
    "    index = 0\n",
    "    for sentence in tqdm(train):\n",
    "        parse_sentence = [morph.parse(form.word)[0] for form in sentence]\n",
    "        for i, form in enumerate(sentence):\n",
    "            samples[index] = get_context_features(i, parse_sentence, context_len)\n",
    "            gram = process_gram_tag(form.gram)\n",
    "            answers[index] = vectorizer_output.get_index_by_name(form.pos + '#' + gram)\n",
    "            index += 1\n",
    "    np.save(TRAIN_SAMPLES_PATH, samples)\n",
    "    np.save(ANSWERS_PATH, answers)\n",
    "else:\n",
    "    samples = np.load(TRAIN_SAMPLES_PATH)\n",
    "    answers = np.load(ANSWERS_PATH)\n",
    "print(samples[0])\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Выбор классификатора\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=100, subsample=0.5, max_features='sqrt', verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Кросс-валидация\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# X, y = samples[:20000], answers[:20000]\n",
    "\n",
    "# scores = cross_val_score(clf, X, y, cv=5)\n",
    "# print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Загрузка тестовой выборки\n",
    "TEST_SAMPLES_PATH = 'test_samples.npy'\n",
    "ANSWERS_PATH = 'answers.npy'\n",
    "if not os.path.exists(TEST_SAMPLES_PATH):\n",
    "    n = sum([1 for sentence in test for word in sentence])\n",
    "    test_samples = np.zeros((n, context_len * vectorizer.grammemes_count()), dtype='bool_')\n",
    "    index = 0\n",
    "    for sentence in tqdm(test):\n",
    "        parse_sentence = [morph.parse(word)[0] for word in sentence]\n",
    "        for i, word in enumerate(sentence):\n",
    "            test_samples[index] = get_context_features(i, parse_sentence, context_len)\n",
    "            index += 1\n",
    "    np.save(TEST_SAMPLES_PATH, test_samples)\n",
    "else:\n",
    "    test_samples = np.load(TEST_SAMPLES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 1241770617093583081706277375487852441548866426442350592.0000 -1236805383171827736670379958583511400814637804995739648.0000        24304.58m\n",
      "         2 1238988731126539267398955232089375060583774307158589440.0000           0.0000        24357.85m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\con3.6\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[1;32m-> 1034\u001b[1;33m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1035\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\con3.6\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1087\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[0;32m   1088\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1089\u001b[1;33m                                      X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\con3.6\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m             residual = loss.negative_gradient(y, y_pred, k=k,\n\u001b[1;32m--> 762\u001b[1;33m                                               sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[1;31m# induce regression tree on residuals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\con3.6\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mnegative_gradient\u001b[1;34m(self, y, pred, k, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[1;34m\"\"\"Compute negative gradient for the ``k``-th class. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m         return y - np.nan_to_num(np.exp(pred[:, k] -\n\u001b[1;32m--> 562\u001b[1;33m                                         logsumexp(pred, axis=1)))\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\con3.6\\lib\\site-packages\\scipy\\special\\_logsumexp.py\u001b[0m in \u001b[0;36mlogsumexp\u001b[1;34m(a, axis, b, keepdims, return_sign)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0ma_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0ma_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;31m# suppress warnings about log of zero\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Обучение классификатора.\n",
    "X, y = samples, answers\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'tree_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-830105d1bdcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mn_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_samples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0manswers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_batches\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\con3.6\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1530\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m         \"\"\"\n\u001b[1;32m-> 1532\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1533\u001b[0m         \u001b[0mdecisions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_score_to_decision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1534\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecisions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\con3.6\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1485\u001b[0m         \"\"\"\n\u001b[0;32m   1486\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1487\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1488\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1489\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\con3.6\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1129\u001b[0m         \u001b[1;31m# not doing input validation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m         \u001b[0mpredict_stages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\ensemble\\_gradient_boosting.pyx\u001b[0m in \u001b[0;36msklearn.ensemble._gradient_boosting.predict_stages\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'tree_'"
     ]
    }
   ],
   "source": [
    "# Предсказания.\n",
    "answers = []\n",
    "batch_size = 1000\n",
    "n_batches = len(test_samples) // batch_size\n",
    "for i in range(n_batches):\n",
    "    answers.extend(list(clf.predict(test_samples[i * batch_size: i * batch_size + batch_size])))\n",
    "answers.extend(list(clf.predict(test_samples[n_batches * batch_size:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сохранение посылки\n",
    "with open('subm.csv', 'w') as f: \n",
    "    f.write('Id,Prediction\\n')\n",
    "    for index, answer in enumerate(answers):\n",
    "        f.write(str(index) + ',' + vectorizer_output.get_name_by_index(answer) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
