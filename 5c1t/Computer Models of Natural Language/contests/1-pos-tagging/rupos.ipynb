{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution is based on the notebook by IlyaGusev:\n",
    "https://github.com/IlyaGusev/nlp-practice/blob/master/rupos.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Имена файлов с данными.\n",
    "from os.path import join\n",
    "PATH_DATA = 'data'\n",
    "TRAIN_FILENAME = join(PATH_DATA, 'train.csv')\n",
    "TEST_FILENAME = join(PATH_DATA, 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Считывание файлов.\n",
    "from collections import namedtuple\n",
    "WordForm = namedtuple('WordForm', 'word pos gram')\n",
    "\n",
    "def get_sentences(filename, is_train):\n",
    "    with open(filename, 'r', encoding='utf-8') as r:\n",
    "        sentence = []\n",
    "        for line in r:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                if len(sentence) > 0:\n",
    "                    yield sentence\n",
    "                    sentence = []\n",
    "                continue\n",
    "            if is_train:\n",
    "                line_parts = line.split('\\t')\n",
    "                word = line_parts[2]\n",
    "                pos, gram = line_parts[3].split('#')\n",
    "                sentence.append(WordForm(word, pos, gram))\n",
    "            else:\n",
    "                word = line.split('\\t')[2]\n",
    "                sentence.append(word)\n",
    "        if len(sentence) != 0:\n",
    "            yield sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = list(get_sentences(TRAIN_FILENAME, True))\n",
    "test = list(get_sentences(TEST_FILENAME, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Класс для удобной векторизации грамматических значений.\n",
    "import jsonpickle\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "def process_gram_tag(gram: str):\n",
    "    gram = gram.strip().split('|')\n",
    "    return '|'.join(sorted(gram))\n",
    "\n",
    "\n",
    "def get_empty_category():\n",
    "    return {GrammemeVectorizer.UNKNOWN_VALUE}\n",
    "\n",
    "\n",
    "class GrammemeVectorizer(object):\n",
    "    UNKNOWN_VALUE = 'Unknown'\n",
    "\n",
    "    def __init__(self, dump_filename: str):\n",
    "        self.all_grammemes = defaultdict(get_empty_category)  # type: Dict[str, Set]\n",
    "        self.vectors = []  # type: List[List[int]]\n",
    "        self.name_to_index = {}  # type: Dict[str, int]\n",
    "        self.dump_filename = dump_filename  # type: str\n",
    "        if os.path.exists(self.dump_filename):\n",
    "            self.load()\n",
    "\n",
    "    def add_grammemes(self, pos_tag: str, gram: str) -> int:\n",
    "        gram = process_gram_tag(gram)\n",
    "        vector_name = pos_tag + '#' + gram\n",
    "        if vector_name not in self.name_to_index:\n",
    "            self.name_to_index[vector_name] = len(self.name_to_index)\n",
    "            self.all_grammemes['POS'].add(pos_tag)\n",
    "            gram = gram.split('|') if gram != '_' else []\n",
    "            for grammeme in gram:\n",
    "                category, value = grammeme.split('=')\n",
    "                self.all_grammemes[category].add(value)\n",
    "        return self.name_to_index[vector_name]\n",
    "\n",
    "    def init_possible_vectors(self) -> None:\n",
    "        self.vectors = []\n",
    "        for grammar_val, index in sorted(self.name_to_index.items(), key=lambda x: x[1]):\n",
    "            pos_tag, grammemes = grammar_val.split('#')\n",
    "            grammemes = grammemes.split('|') if grammemes != '_' else []\n",
    "            vector = self.__build_vector(pos_tag, grammemes)\n",
    "            self.vectors.append(vector)\n",
    "\n",
    "    def get_vector(self, vector_name: str) -> List[int]:\n",
    "        if vector_name not in self.name_to_index:\n",
    "            return [0] * len(self.vectors[0])\n",
    "        return self.vectors[self.name_to_index[vector_name]]\n",
    "\n",
    "    def get_vector_by_index(self, index: int) -> List[int]:\n",
    "        return self.vectors[index] if 0 <= index < len(self.vectors) else [0] * len(self.vectors[0])\n",
    "\n",
    "    def get_ordered_grammemes(self) -> List[str]:\n",
    "        flat = []\n",
    "        sorted_grammemes = sorted(self.all_grammemes.items(), key=lambda x: x[0])\n",
    "        for category, values in sorted_grammemes:\n",
    "            for value in sorted(list(values)):\n",
    "                flat.append(category + '=' + value)\n",
    "        return flat\n",
    "    \n",
    "    def save(self) -> None:\n",
    "        with open(self.dump_filename, 'w') as f:\n",
    "            f.write(jsonpickle.encode(self, f))\n",
    "\n",
    "    def load(self):\n",
    "        with open(self.dump_filename, 'r') as f:\n",
    "            vectorizer = jsonpickle.decode(f.read())\n",
    "            self.__dict__.update(vectorizer.__dict__)\n",
    "\n",
    "    def size(self) -> int:\n",
    "        return len(self.vectors)\n",
    "\n",
    "    def grammemes_count(self) -> int:\n",
    "        return len(self.get_ordered_grammemes())\n",
    "\n",
    "    def is_empty(self) -> int:\n",
    "        return len(self.vectors) == 0\n",
    "\n",
    "    def get_name_by_index(self, index):\n",
    "        d = {index: name for name, index in self.name_to_index.items()}\n",
    "        return d[index]\n",
    "\n",
    "    def get_index_by_name(self, name):\n",
    "        pos = name.split('#')[0]\n",
    "        gram = process_gram_tag(name.split('#')[1])\n",
    "        return self.name_to_index[pos + '#' + gram]\n",
    "\n",
    "    def __build_vector(self, pos_tag: str, grammemes: List[str]) -> List[int]:\n",
    "        vector = []\n",
    "        gram_tags = {pair.split('=')[0]: pair.split('=')[1] for pair in grammemes}\n",
    "        gram_tags['POS'] = pos_tag\n",
    "        sorted_grammemes = sorted(self.all_grammemes.items(), key=lambda x: x[0])\n",
    "        for category, values in sorted_grammemes:\n",
    "            value_correct = gram_tags[category] if category in gram_tags else GrammemeVectorizer.UNKNOWN_VALUE\n",
    "            vector.extend(1 if value == value_correct else 0 for value in sorted(list(values)))\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from russian_tagsets import converters\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "to_ud = converters.converter('opencorpora-int', 'ud14')\n",
    "\n",
    "def convert_from_opencorpora_tag(tag, text):\n",
    "    ud_tag = to_ud(str(tag), text)\n",
    "    pos, gram = ud_tag.split()\n",
    "    return pos, gram\n",
    "\n",
    "def fill_all_variants(word, vectorizer):\n",
    "    for parse in morph.parse(word):\n",
    "        pos, gram = convert_from_opencorpora_tag(parse.tag, parse.word)\n",
    "        vectorizer.add_grammemes(pos, gram)\n",
    "\n",
    "vectorizer = GrammemeVectorizer('vectorizer.json')\n",
    "if vectorizer.is_empty():\n",
    "    print('Add train sentences to vectorizer')\n",
    "    for sentence in tqdm(train):\n",
    "        for form in sentence:\n",
    "            fill_all_variants(form.word, vectorizer)\n",
    "    print('Add test sentences to vectorizer')\n",
    "    for sentence in tqdm(test):\n",
    "        for word in sentence:\n",
    "            fill_all_variants(word, vectorizer)\n",
    "    print('Init vectors in vectorizer')\n",
    "    vectorizer.init_possible_vectors()\n",
    "    vectorizer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_output = GrammemeVectorizer('vectorizer_output.json')\n",
    "if vectorizer_output.is_empty():\n",
    "    for sentence in tqdm(train):\n",
    "        for form in sentence:\n",
    "            vectorizer_output.add_grammemes(form.pos, gram)\n",
    "    vectorizer_output.init_possible_vectors()\n",
    "    vectorizer_output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Получение признаков для конкретного контекста.\n",
    "def get_context_features(i, parse_sentence, context_len):\n",
    "    sample = []\n",
    "    left = i - (context_len - 1) // 2\n",
    "    right = i + context_len // 2\n",
    "    if left < 0:\n",
    "        sample.extend(0 for _ in range(vectorizer.grammemes_count() * (-left)))\n",
    "    for parse in parse_sentence[max(left, 0): min(right + 1, len(sentence))]:\n",
    "        pos, gram = convert_from_opencorpora_tag(parse.tag, parse.word)\n",
    "        gram = process_gram_tag(gram)\n",
    "        sample.extend(vectorizer.get_vector(pos + '#' + gram))\n",
    "    if right > len(sentence) - 1:\n",
    "        sample.extend(0 for _ in range(vectorizer.grammemes_count() * (right - len(sentence) + 1)))\n",
    "    assert len(sample) == context_len * vectorizer.grammemes_count()\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(850689, 310)\n",
      "(850689,)\n"
     ]
    }
   ],
   "source": [
    "# Загрузка обучающей выборки.\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "context_len = 5\n",
    "\n",
    "TRAIN_SAMPLES_PATH = 'samples.npy'\n",
    "ANSWERS_PATH = 'answers.npy'\n",
    "if not os.path.exists(TRAIN_SAMPLES_PATH) or not os.path.exists(ANSWERS_PATH):\n",
    "    n = sum([1 for sentence in train for word in sentence])\n",
    "    samples = np.zeros((n, context_len * vectorizer.grammemes_count()), dtype='bool_')\n",
    "    answers = np.zeros((n, ), dtype='int')\n",
    "    index = 0\n",
    "    for sentence in tqdm(train):\n",
    "        parse_sentence = [morph.parse(form.word)[0] for form in sentence]\n",
    "        for i, form in enumerate(sentence):\n",
    "            samples[index] = get_context_features(i, parse_sentence, context_len)\n",
    "            gram = process_gram_tag(form.gram)\n",
    "            answers[index] = vectorizer_output.get_index_by_name(form.pos + '#' + gram)\n",
    "            index += 1\n",
    "    np.save(TRAIN_SAMPLES_PATH, samples)\n",
    "    np.save(ANSWERS_PATH, answers)\n",
    "else:\n",
    "    samples = np.load(TRAIN_SAMPLES_PATH)\n",
    "    answers = np.load(ANSWERS_PATH)\n",
    "print(samples.shape)\n",
    "print(answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Выбор классификатора\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# clf = GradientBoostingClassifier(n_estimators=100, subsample=0.5, max_features='sqrt', verbose=2)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASautin\\AppData\\Local\\Continuum\\Anaconda2\\envs\\con3.6\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.76321454  0.77856967  0.79543173  0.8121011   0.81034035]\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Кросс-валидация\n",
    "from sklearn.model_selection import cross_val_score\n",
    "X, y = samples[:20000], answers[:20000]\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Загрузка тестовой выборки\n",
    "TEST_SAMPLES_PATH = 'test_samples.npy'\n",
    "ANSWERS_PATH = 'answers.npy'\n",
    "if not os.path.exists(TEST_SAMPLES_PATH):\n",
    "    n = sum([1 for sentence in test for word in sentence])\n",
    "    test_samples = np.zeros((n, context_len * vectorizer.grammemes_count()), dtype='bool_')\n",
    "    index = 0\n",
    "    for sentence in tqdm(test):\n",
    "        parse_sentence = [morph.parse(word)[0] for word in sentence]\n",
    "        for i, word in enumerate(sentence):\n",
    "            test_samples[index] = get_context_features(i, parse_sentence, context_len)\n",
    "            index += 1\n",
    "    np.save(TEST_SAMPLES_PATH, test_samples)\n",
    "else:\n",
    "    test_samples = np.load(TEST_SAMPLES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Обучение классификатора.\n",
    "X, y = samples[:20000], answers[:20000]\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказания.\n",
    "answers = []\n",
    "batch_size = 1000\n",
    "n_batches = len(test_samples) // batch_size\n",
    "for i in range(n_batches):\n",
    "    answers.extend(list(clf.predict(test_samples[i * batch_size: i * batch_size + batch_size])))\n",
    "answers.extend(list(clf.predict(test_samples[n_batches * batch_size:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сохранение посылки\n",
    "with open('subm.csv', 'w') as f: \n",
    "    f.write('Id,Prediction\\n')\n",
    "    for index, answer in enumerate(answers):\n",
    "        f.write(str(index) + ',' + vectorizer_output.get_name_by_index(answer) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
