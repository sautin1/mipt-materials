{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution is based on the notebook by IlyaGusev:\n",
    "https://github.com/IlyaGusev/nlp-practice/blob/master/rupos.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Имена файлов с данными.\n",
    "from os.path import join\n",
    "PATH_DATA = 'data'\n",
    "TRAIN_FILENAME = join(PATH_DATA, 'train.csv')\n",
    "TEST_FILENAME = join(PATH_DATA, 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Считывание файлов.\n",
    "from collections import namedtuple\n",
    "WordForm = namedtuple('WordForm', 'word pos gram')\n",
    "\n",
    "def get_sentences(filename, is_train):\n",
    "    with open(filename, 'r', encoding='utf-8') as r:\n",
    "        sentence = []\n",
    "        for line in r:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                if len(sentence) > 0:\n",
    "                    yield sentence\n",
    "                    sentence = []\n",
    "                continue\n",
    "            if is_train:\n",
    "                line_parts = line.split('\\t')\n",
    "                word = line_parts[2]\n",
    "                pos, gram = line_parts[3].split('#')\n",
    "                sentence.append(WordForm(word, pos, gram))\n",
    "            else:\n",
    "                word = line.split('\\t')[2]\n",
    "                sentence.append(word)\n",
    "        if len(sentence) != 0:\n",
    "            yield sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = list(get_sentences(TRAIN_FILENAME, True))\n",
    "test = list(get_sentences(TEST_FILENAME, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Класс для удобной векторизации грамматических значений.\n",
    "import jsonpickle\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "def process_gram_tag(gram: str):\n",
    "    gram = gram.strip().split('|')\n",
    "    return '|'.join(sorted(gram))\n",
    "\n",
    "\n",
    "def get_empty_category():\n",
    "    return {GrammemeVectorizer.UNKNOWN_VALUE}\n",
    "\n",
    "\n",
    "class GrammemeVectorizer(object):\n",
    "    UNKNOWN_VALUE = 'Unknown'\n",
    "\n",
    "    def __init__(self, dump_filename: str):\n",
    "        self.all_grammemes = defaultdict(get_empty_category)  # type: Dict[str, Set]\n",
    "        self.vectors = []  # type: List[List[int]]\n",
    "        self.name_to_index = {}  # type: Dict[str, int]\n",
    "        self.dump_filename = dump_filename  # type: str\n",
    "        if os.path.exists(self.dump_filename):\n",
    "            self.load()\n",
    "\n",
    "    def add_grammemes(self, pos_tag: str, gram: str) -> int:\n",
    "        gram = process_gram_tag(gram)\n",
    "        vector_name = pos_tag + '#' + gram\n",
    "        if vector_name not in self.name_to_index:\n",
    "            self.name_to_index[vector_name] = len(self.name_to_index)\n",
    "            self.all_grammemes['POS'].add(pos_tag)\n",
    "            gram = gram.split('|') if gram != '_' else []\n",
    "            for grammeme in gram:\n",
    "                category, value = grammeme.split('=')\n",
    "                self.all_grammemes[category].add(value)\n",
    "        return self.name_to_index[vector_name]\n",
    "\n",
    "    def init_possible_vectors(self) -> None:\n",
    "        self.vectors = []\n",
    "        for grammar_val, index in sorted(self.name_to_index.items(), key=lambda x: x[1]):\n",
    "            pos_tag, grammemes = grammar_val.split('#')\n",
    "            grammemes = grammemes.split('|') if grammemes != '_' else []\n",
    "            vector = self.__build_vector(pos_tag, grammemes)\n",
    "            self.vectors.append(vector)\n",
    "\n",
    "    def get_vector(self, vector_name: str) -> List[int]:\n",
    "        if vector_name not in self.name_to_index:\n",
    "            return [0] * len(self.vectors[0])\n",
    "        return self.vectors[self.name_to_index[vector_name]]\n",
    "\n",
    "    def get_vector_by_index(self, index: int) -> List[int]:\n",
    "        return self.vectors[index] if 0 <= index < len(self.vectors) else [0] * len(self.vectors[0])\n",
    "\n",
    "    def get_ordered_grammemes(self) -> List[str]:\n",
    "        flat = []\n",
    "        sorted_grammemes = sorted(self.all_grammemes.items(), key=lambda x: x[0])\n",
    "        for category, values in sorted_grammemes:\n",
    "            for value in sorted(list(values)):\n",
    "                flat.append(category + '=' + value)\n",
    "        return flat\n",
    "    \n",
    "    def save(self) -> None:\n",
    "        with open(self.dump_filename, 'w') as f:\n",
    "            f.write(jsonpickle.encode(self, f))\n",
    "\n",
    "    def load(self):\n",
    "        with open(self.dump_filename, 'r') as f:\n",
    "            vectorizer = jsonpickle.decode(f.read())\n",
    "            self.__dict__.update(vectorizer.__dict__)\n",
    "\n",
    "    def size(self) -> int:\n",
    "        return len(self.vectors)\n",
    "\n",
    "    def grammemes_count(self) -> int:\n",
    "        return len(self.get_ordered_grammemes())\n",
    "\n",
    "    def is_empty(self) -> int:\n",
    "        return len(self.vectors) == 0\n",
    "\n",
    "    def get_name_by_index(self, index):\n",
    "        d = {index: name for name, index in self.name_to_index.items()}\n",
    "        return d[index]\n",
    "\n",
    "    def get_index_by_name(self, name):\n",
    "        pos = name.split('#')[0]\n",
    "        gram = process_gram_tag(name.split('#')[1])\n",
    "        return self.name_to_index[pos + '#' + gram]\n",
    "\n",
    "    def __build_vector(self, pos_tag: str, grammemes: List[str]) -> List[int]:\n",
    "        vector = []\n",
    "        gram_tags = {pair.split('=')[0]: pair.split('=')[1] for pair in grammemes}\n",
    "        gram_tags['POS'] = pos_tag\n",
    "        sorted_grammemes = sorted(self.all_grammemes.items(), key=lambda x: x[0])\n",
    "        for category, values in sorted_grammemes:\n",
    "            value_correct = gram_tags[category] if category in gram_tags else GrammemeVectorizer.UNKNOWN_VALUE\n",
    "            vector.extend(1 if value == value_correct else 0 for value in sorted(list(values)))\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from russian_tagsets import converters\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "to_ud = converters.converter('opencorpora-int', 'ud14')\n",
    "\n",
    "def convert_from_opencorpora_tag(tag, text):\n",
    "    ud_tag = to_ud(str(tag), text)\n",
    "    pos, gram = ud_tag.split()\n",
    "    return pos, gram\n",
    "\n",
    "def fill_all_variants(word, vectorizer):\n",
    "    for parse in morph.parse(word):\n",
    "        pos, gram = convert_from_opencorpora_tag(parse.tag, parse.word)\n",
    "        vectorizer.add_grammemes(pos, gram)\n",
    "\n",
    "vectorizer = GrammemeVectorizer('vectorizer.json')\n",
    "if vectorizer.is_empty():\n",
    "    print('Add train sentences to vectorizer')\n",
    "    for sentence in tqdm(train):\n",
    "        for form in sentence:\n",
    "            fill_all_variants(form.word, vectorizer)\n",
    "    print('Add test sentences to vectorizer')\n",
    "    for sentence in tqdm(test):\n",
    "        for word in sentence:\n",
    "            fill_all_variants(word, vectorizer)\n",
    "    print('Init vectors in vectorizer')\n",
    "    vectorizer.init_possible_vectors()\n",
    "    vectorizer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_output = GrammemeVectorizer('vectorizer_output.json')\n",
    "if vectorizer_output.is_empty():\n",
    "    for sentence in tqdm(train):\n",
    "        for form in sentence:\n",
    "            vectorizer_output.add_grammemes(form.pos, gram)\n",
    "    vectorizer_output.init_possible_vectors()\n",
    "    vectorizer_output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Получение признаков для конкретного контекста.\n",
    "def get_context_features(i, parse_sentence, context_len):\n",
    "    sample = []\n",
    "    left = i - (context_len - 1) // 2\n",
    "    right = i + context_len // 2\n",
    "    if left < 0:\n",
    "        sample.extend(0 for _ in range(vectorizer.grammemes_count() * (-left)))\n",
    "    for parse in parse_sentence[max(left, 0): min(right + 1, len(sentence))]:\n",
    "        pos, gram = convert_from_opencorpora_tag(parse.tag, parse.word)\n",
    "        gram = process_gram_tag(gram)\n",
    "        sample.extend(vectorizer.get_vector(pos + '#' + gram))\n",
    "    if right > len(sentence) - 1:\n",
    "        sample.extend(0 for _ in range(vectorizer.grammemes_count() * (right - len(sentence) + 1)))\n",
    "    assert len(sample) == context_len * vectorizer.grammemes_count()\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Загрузка обучающей выборки.\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "context_len = 5\n",
    "\n",
    "TRAIN_SAMPLES_PATH = 'samples.npy'\n",
    "ANSWERS_PATH = 'answers.npy'\n",
    "if not os.path.exists(TRAIN_SAMPLES_PATH) or not os.path.exists(ANSWERS_PATH):\n",
    "    n = sum([1 for sentence in train for word in sentence])\n",
    "    samples = np.zeros((n, context_len * vectorizer.grammemes_count()), dtype='bool_')\n",
    "    answers = np.zeros((n, ), dtype='int')\n",
    "    index = 0\n",
    "    for sentence in tqdm(train):\n",
    "        parse_sentence = [morph.parse(form.word)[0] for form in sentence]\n",
    "        for i, form in enumerate(sentence):\n",
    "            samples[index] = get_context_features(i, parse_sentence, context_len)\n",
    "            gram = process_gram_tag(form.gram)\n",
    "            answers[index] = vectorizer_output.get_index_by_name(form.pos + '#' + gram)\n",
    "            index += 1\n",
    "    np.save(TRAIN_SAMPLES_PATH, samples)\n",
    "    np.save(ANSWERS_PATH, answers)\n",
    "else:\n",
    "    samples = np.load(TRAIN_SAMPLES_PATH)\n",
    "    answers = np.load(ANSWERS_PATH)\n",
    "class_count = len(np.unique(answers))\n",
    "class_distribution = Counter(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples.shape = (850689, 310)\n",
      "answers.shape = (850689,)\n",
      "class_count = 581\n",
      "Counter({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})\n"
     ]
    }
   ],
   "source": [
    "print(f'samples.shape = {samples.shape}')\n",
    "print(f'answers.shape = {answers.shape}')\n",
    "print(f'class_count = {class_count}')\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Oversampling\n",
    "# CLASS_LIMIT = 1500\n",
    "\n",
    "# pairs_sorted = sorted(zip(samples, answers), key=lambda pair: pair[1])\n",
    "# samples_balanced, answers_balanced = [], []\n",
    "# answer_previous = -1\n",
    "# for sample, answer in tqdm(pairs_sorted):\n",
    "#     if answer != answer_previous:\n",
    "#         answer_previous = answer\n",
    "#         count_without_repeat = min(CLASS_LIMIT, class_distribution[answer])\n",
    "#         count_repeat = CLASS_LIMIT // count_without_repeat\n",
    "#         count_remainder = CLASS_LIMIT % count_without_repeat\n",
    "#         entry_count = 0\n",
    "#     if entry_count < CLASS_LIMIT:\n",
    "#         repeats = count_repeat\n",
    "#         if count_remainder > 0:\n",
    "#             count_remainder -= 1\n",
    "#             repeats += 1\n",
    "#         samples_balanced.extend([sample] * repeats)\n",
    "#         answers_balanced.extend([answer] * repeats)\n",
    "#         entry_count += repeats\n",
    "# samples, answers = np.array(samples_balanced), np.array(answers_balanced)\n",
    "# indices = np.random.permutation(samples.shape[0])\n",
    "# samples, answers = samples[indices], answers[indices]\n",
    "# print(samples.shape)\n",
    "# print(answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 310)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              318464    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 581)               149317    \n",
      "=================================================================\n",
      "Total params: 1,386,565\n",
      "Trainable params: 1,386,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Выбор классификатора\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.regularizers import l2\n",
    "\n",
    "inp = Input(shape=(samples.shape[1],))\n",
    "x = Dense(1024, activation='relu')(inp)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Dense(class_count, activation='softmax')(x)\n",
    "\n",
    "clf = Model(inputs=[inp], outputs=[x])\n",
    "\n",
    "clf.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Загрузка тестовой выборки\n",
    "TEST_SAMPLES_PATH = 'test_samples.npy'\n",
    "ANSWERS_PATH = 'answers.npy'\n",
    "if not os.path.exists(TEST_SAMPLES_PATH):\n",
    "    n = sum([1 for sentence in test for word in sentence])\n",
    "    test_samples = np.zeros((n, context_len * vectorizer.grammemes_count()), dtype='bool_')\n",
    "    index = 0\n",
    "    for sentence in tqdm(test):\n",
    "        parse_sentence = [morph.parse(word)[0] for word in sentence]\n",
    "        for i, word in enumerate(sentence):\n",
    "            test_samples[index] = get_context_features(i, parse_sentence, context_len)\n",
    "            index += 1\n",
    "    np.save(TEST_SAMPLES_PATH, test_samples)\n",
    "else:\n",
    "    test_samples = np.load(TEST_SAMPLES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import itertools\n",
    "\n",
    "X, y = samples.astype(np.float64), to_categorical(answers, num_classes=class_count)\n",
    "\n",
    "\n",
    "def repeat_func(func, times=None, *args):\n",
    "    if times is None:\n",
    "        return itertools.starmap(func, itertools.repeat(args))\n",
    "    return itertools.starmap(func, itertools.repeat(args, times))\n",
    "\n",
    "\n",
    "def repeat_infinitely(func, *args):\n",
    "    return itertools.chain.from_iterable(repeat_func(func, None, *args))\n",
    "\n",
    "\n",
    "def collect_batches(gen, batch_size=32, randomize=True, probability=0.5):\n",
    "    while True:\n",
    "        batch_x, batch_y = [], []\n",
    "        while len(batch_x) < batch_size:\n",
    "            x, y = next(gen)\n",
    "            if randomize and np.random.rand() < probability or not randomize:\n",
    "                batch_x.append(x)\n",
    "                batch_y.append(y)\n",
    "        yield np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "\n",
    "def init_data_generator(xs, ys):\n",
    "    gen = repeat_infinitely(zip, xs, ys)\n",
    "    return collect_batches(gen, randomize=False)\n",
    "\n",
    "gen_data = init_data_generator(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 49s - loss: 1.4565 - acc: 0.7116    \n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 50s - loss: 1.7655 - acc: 0.7127    \n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 50s - loss: 2.0568 - acc: 0.7061    \n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 50s - loss: 2.1650 - acc: 0.7037    \n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 50s - loss: 2.2722 - acc: 0.6966    \n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 50s - loss: 2.4402 - acc: 0.6872    \n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 51s - loss: 2.5303 - acc: 0.6860    \n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 50s - loss: 2.6901 - acc: 0.6669    \n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 50s - loss: 2.7291 - acc: 0.6584    \n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 51s - loss: 2.8622 - acc: 0.6323    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fb5f6b6320>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучение классификатора.\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau, LambdaCallback\n",
    "\n",
    "clf.fit_generator(gen_data,\n",
    "                  steps_per_epoch=10000,\n",
    "                  epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217794\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Предсказания.\n",
    "test_samples_float = test_samples.astype(np.float64)\n",
    "\n",
    "answers = []\n",
    "batch_size = 1000\n",
    "n_batches = len(test_samples) // batch_size\n",
    "for i in range(n_batches):\n",
    "    predictions = clf.predict(test_samples_float[i * batch_size: i * batch_size + batch_size])\n",
    "    predictions = list(clf.predict(test_samples_float[i * batch_size: i * batch_size + batch_size]))\n",
    "    answers.extend(list(map(np.argmax, predictions)))\n",
    "predictions = list(clf.predict(test_samples_float[n_batches * batch_size:]))\n",
    "answers.extend(list(map(np.argmax, predictions)))\n",
    "\n",
    "print(len(answers))\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сохранение посылки\n",
    "with open('subm.csv', 'w') as f: \n",
    "    f.write('Id,Prediction\\n')\n",
    "    for index, answer in enumerate(answers):\n",
    "        f.write(str(index) + ',' + vectorizer_output.get_name_by_index(answer) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:con3.6]",
   "language": "python",
   "name": "conda-env-con3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
