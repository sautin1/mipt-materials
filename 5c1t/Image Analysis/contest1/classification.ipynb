{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input, Conv2D, Dropout, MaxPool2D, GlobalAveragePooling2D, GlobalMaxPooling2D, LeakyReLU\n",
    "from keras.layers import concatenate, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from paths import PATH_DATA, PATH_PROJECT\n",
    "from iterators import roundrobin, repeat_infinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DarknetBlock:\n",
    "    def __init__(self, filters1, filters3, strides=(1, 1)):\n",
    "        self._filters1 = filters1\n",
    "        self._filters3 = filters3\n",
    "        self._strides = strides\n",
    "        \n",
    "    def __call__(self, input_layer, *args, **kwargs):\n",
    "        x = input_layer\n",
    "        for filters, kernel, strides in zip([self._filters1, self._filters3],\n",
    "                                            [(1, 1), (3, 3)],\n",
    "                                            [(1, 1), self._strides]):\n",
    "            x = Conv2D(filters, kernel, padding='same', strides=strides)(x)\n",
    "            x = LeakyReLU()(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def build_model(class_count, is_train=False):\n",
    "    input_tensor = Input(shape=(1, None, None))\n",
    "    x = Conv2D(10, (3, 3), strides=(2, 2), padding='same')(input_tensor)\n",
    "    filters = [20, 30, 40]\n",
    "    for idx, filter_size in enumerate(filters):\n",
    "        x = DarknetBlock(filter_size, filter_size * 2)(x)\n",
    "        x = DarknetBlock(filter_size, filter_size * 2)(x)\n",
    "        if idx < len(filters) - 1:\n",
    "            x = MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
    "            if is_train:\n",
    "                x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Conv2D(filters[-1], (1, 1), padding='same')(x)\n",
    "    max_output = GlobalMaxPooling2D()(x)\n",
    "    average_output = GlobalAveragePooling2D()(x)\n",
    "    x = concatenate(inputs=[max_output, average_output], axis=1)\n",
    "    x = Dense(class_count, activation='softmax')(x)\n",
    "    return Model(inputs=[input_tensor], outputs=[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLASS_COUNT = 6\n",
    "# cannot make bigger batches because images are of different size\n",
    "BATCH_SIZE = 1\n",
    "TRAIN_SIZE = 7000 * 80 // 100\n",
    "VAL_SIZE = 7000 - TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_label_map(path):\n",
    "    with open(path, 'r') as fin:\n",
    "        reader = csv.reader(fin, delimiter=',')\n",
    "        # ignore header\n",
    "        next(reader)\n",
    "        label_map = {filename: int(label) for filename, label in reader}\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def get_images_paths(root, label_count):\n",
    "    dir_paths = list(map(lambda label: join(root, str(label)), range(label_count)))\n",
    "    file_paths = [list(map(lambda filename: join(dir_path, filename), os.listdir(dir_path))) for dir_path in dir_paths]\n",
    "    return roundrobin(*file_paths)\n",
    "\n",
    "\n",
    "def get_train_data(path_images, label_count, label_map):\n",
    "    paths = repeat_infinitely(get_images_paths, path_images, label_count)\n",
    "    for path in paths:\n",
    "        image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f'Image cannot be read: {path}')\n",
    "        yield (np.expand_dims(image, 0),\n",
    "               to_categorical(label_map[os.path.basename(path)], label_count)[0])\n",
    "\n",
    "\n",
    "def collect_batches(iterable, batch_size=32, randomize=False, probability=0.5, rotate=False):\n",
    "    while True:\n",
    "        images, labels = [], []\n",
    "        while len(images) < batch_size:\n",
    "            image, label = next(iterable)\n",
    "            if not randomize or np.random.rand() < probability:\n",
    "                if rotate:\n",
    "                    image = np.rot90(image, np.random.randint(0, 4), (1, 2))\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "        yield np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_map = read_label_map(join(PATH_DATA, 'train_labels.csv'))\n",
    "gen_train_data = get_train_data(join(PATH_DATA, 'train'), CLASS_COUNT, label_map)\n",
    "gen_train_batches = collect_batches(gen_train_data, BATCH_SIZE, randomize=True, rotate=True)\n",
    "\n",
    "gen_val_data = get_train_data(join(PATH_DATA, 'validation'), CLASS_COUNT, label_map)\n",
    "gen_val_batches = collect_batches(gen_val_data, BATCH_SIZE, randomize=False, rotate=False)\n",
    "\n",
    "# x, y = next(gen_val_batches)\n",
    "# print(x.shape)\n",
    "# print(y)\n",
    "# \n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.imshow(x[0, 0], cmap='Greys_r')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 1, None, None) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 10, None, None 100         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 20, None, None 220         conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)        (None, 20, None, None 0           conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 40, None, None 7240        leaky_re_lu_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)        (None, 40, None, None 0           conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 20, None, None 820         leaky_re_lu_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)        (None, 20, None, None 0           conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 40, None, None 7240        leaky_re_lu_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)        (None, 40, None, None 0           conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 40, None, None 0           leaky_re_lu_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 40, None, None 0           max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 30, None, None 1230        dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)        (None, 30, None, None 0           conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 60, None, None 16260       leaky_re_lu_5[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)        (None, 60, None, None 0           conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 30, None, None 1830        leaky_re_lu_6[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)        (None, 30, None, None 0           conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 60, None, None 16260       leaky_re_lu_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)        (None, 60, None, None 0           conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)   (None, 60, None, None 0           leaky_re_lu_8[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 60, None, None 0           max_pooling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 40, None, None 2440        dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)        (None, 40, None, None 0           conv2d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 80, None, None 28880       leaky_re_lu_9[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)       (None, 80, None, None 0           conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, 40, None, None 3240        leaky_re_lu_10[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)       (None, 40, None, None 0           conv2d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)               (None, 80, None, None 28880       leaky_re_lu_11[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)       (None, 80, None, None 0           conv2d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)               (None, 40, None, None 3240        leaky_re_lu_12[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalMa (None, 40)            0           conv2d_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glob (None, 40)            0           conv2d_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 80)            0           global_max_pooling2d_1[0][0]     \n",
      "                                                                   global_average_pooling2d_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 6)             486         concatenate_1[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 118,366\n",
      "Trainable params: 118,366\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(CLASS_COUNT, is_train=True)\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "5600/5600 [==============================] - 190s - loss: 0.0800 - acc: 0.9727 - val_loss: 3.3490e-04 - val_acc: 1.0000\n",
      "Epoch 2/30\n",
      "5600/5600 [==============================] - 156s - loss: 0.0062 - acc: 0.9987 - val_loss: 4.2460e-07 - val_acc: 1.0000\n",
      "Epoch 3/30\n",
      "5600/5600 [==============================] - 155s - loss: 2.9699e-04 - acc: 0.9998 - val_loss: 1.2164e-07 - val_acc: 1.0000\n",
      "Epoch 4/30\n",
      "5600/5600 [==============================] - 154s - loss: 0.0024 - acc: 0.9996 - val_loss: 1.3928e-04 - val_acc: 1.0000\n",
      "Epoch 5/30\n",
      "5600/5600 [==============================] - 153s - loss: 2.0321e-04 - acc: 1.0000 - val_loss: 7.7401e-06 - val_acc: 1.0000\n",
      "Epoch 6/30\n",
      "5600/5600 [==============================] - 154s - loss: 0.0013 - acc: 0.9995 - val_loss: 6.5688e-07 - val_acc: 1.0000\n",
      "Epoch 7/30\n",
      "5600/5600 [==============================] - 155s - loss: 5.4910e-04 - acc: 0.9998 - val_loss: 1.2070e-07 - val_acc: 1.0000\n",
      "Epoch 8/30\n",
      "5600/5600 [==============================] - 152s - loss: 2.0507e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 9/30\n",
      "5600/5600 [==============================] - 154s - loss: 9.4694e-04 - acc: 0.9998 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 10/30\n",
      "5600/5600 [==============================] - 155s - loss: 2.2953e-04 - acc: 0.9998 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 11/30\n",
      "5600/5600 [==============================] - 156s - loss: 2.5419e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      "5600/5600 [==============================] - 154s - loss: 1.1958e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 13/30\n",
      "5600/5600 [==============================] - 155s - loss: 2.4116e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      "5600/5600 [==============================] - 156s - loss: 7.9804e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "5600/5600 [==============================] - 156s - loss: 3.6095e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 16/30\n",
      "5600/5600 [==============================] - 155s - loss: 8.3928e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "5600/5600 [==============================] - 154s - loss: 3.4590e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "5600/5600 [==============================] - 154s - loss: 0.0013 - acc: 0.9998 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 19/30\n",
      "5600/5600 [==============================] - 153s - loss: 1.3102e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "5600/5600 [==============================] - 157s - loss: 1.1928e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "5600/5600 [==============================] - 156s - loss: 2.0895e-04 - acc: 0.9998 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      "5600/5600 [==============================] - 156s - loss: 7.6016e-04 - acc: 0.9998 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "5600/5600 [==============================] - 155s - loss: 4.4690e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "5600/5600 [==============================] - 157s - loss: 1.2582e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "5600/5600 [==============================] - 156s - loss: 1.7446e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "5600/5600 [==============================] - 157s - loss: 1.2221e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "5600/5600 [==============================] - 155s - loss: 8.7448e-06 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "5600/5600 [==============================] - 154s - loss: 3.0105e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "5600/5600 [==============================] - 157s - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "5600/5600 [==============================] - 154s - loss: 7.9524e-04 - acc: 0.9998 - val_loss: 1.1921e-07 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator=gen_train_batches,\n",
    "                              steps_per_epoch=TRAIN_SIZE,\n",
    "                              epochs=30,\n",
    "                              validation_data=gen_val_batches,\n",
    "                              validation_steps=VAL_SIZE,\n",
    "                              callbacks=[ReduceLROnPlateau(patience=3),\n",
    "                                         ModelCheckpoint(join(PATH_PROJECT, 'backups', 'weights_{epoch:02d}'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_weights = join(PATH_PROJECT, 'weights')\n",
    "model.save_weights(path_weights, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_data(path_images):\n",
    "    paths = map(lambda filename: join(path_images, filename), os.listdir(path_images))\n",
    "    return np.array([list(map(lambda path: cv2.imread(path, cv2.IMREAD_GRAYSCALE), paths))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_weights_best = join(PATH_PROJECT, 'backups', 'weights_13')\n",
    "model_predict = build_model(CLASS_COUNT, is_train=False)\n",
    "model_predict.load_weights(path_weights_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = join(PATH_DATA, 'test')\n",
    "answers = {}\n",
    "for filename in os.listdir(path_test):\n",
    "    file_path = join(path_test, filename)\n",
    "    image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    tensor = image.reshape((1, 1, *image.shape))\n",
    "    label_categorical = model_predict.predict(tensor)\n",
    "    answers[filename] = np.argmax(label_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(PATH_PROJECT, 'submission.csv'), 'w') as fout:\n",
    "    for filename, label in answers.items():\n",
    "        print(filename, label, file=fout, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
