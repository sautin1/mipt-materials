{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input, Conv2D, Dropout, MaxPool2D, GlobalAveragePooling2D, GlobalMaxPooling2D, LeakyReLU\n",
    "from keras.layers import concatenate, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from paths import PATH_DATA, PATH_PROJECT\n",
    "from iterators import roundrobin, repeat_infinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DarknetBlock:\n",
    "    def __init__(self, filters1, filters3, strides=(1, 1)):\n",
    "        self._filters1 = filters1\n",
    "        self._filters3 = filters3\n",
    "        self._strides = strides\n",
    "        \n",
    "    def __call__(self, input_layer, *args, **kwargs):\n",
    "        x = input_layer\n",
    "        for filters, kernel, strides in zip([self._filters1, self._filters3],\n",
    "                                            [(1, 1), (3, 3)],\n",
    "                                            [(1, 1), self._strides]):\n",
    "            x = Conv2D(filters, kernel, padding='same', strides=strides)(x)\n",
    "            x = LeakyReLU()(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def build_model(class_count, is_train=False):\n",
    "    input_tensor = Input(shape=(1, None, None))\n",
    "    x = Conv2D(10, (3, 3), strides=(2, 2), padding='same')(input_tensor)\n",
    "    filters = [20, 30, 40]\n",
    "    for idx, filter_size in enumerate(filters):\n",
    "        x = DarknetBlock(filter_size, filter_size * 2)(x)\n",
    "        x = DarknetBlock(filter_size, filter_size * 2)(x)\n",
    "        if idx < len(filters) - 1:\n",
    "            x = MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
    "            if is_train:\n",
    "                x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Conv2D(filters[-1], (1, 1), padding='same')(x)\n",
    "    max_output = GlobalMaxPooling2D()(x)\n",
    "    average_output = GlobalAveragePooling2D()(x)\n",
    "    x = concatenate(inputs=[max_output, average_output], axis=1)\n",
    "    x = Dense(class_count, activation='softmax')(x)\n",
    "    return Model(inputs=[input_tensor], outputs=[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLASS_COUNT = 6\n",
    "# cannot make bigger batches because images are of different size\n",
    "BATCH_SIZE = 1\n",
    "TRAIN_SIZE = 7000 * 80 // 100\n",
    "VAL_SIZE = 7000 - TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_label_map(path):\n",
    "    with open(path, 'r') as fin:\n",
    "        reader = csv.reader(fin, delimiter=',')\n",
    "        # ignore header\n",
    "        next(reader)\n",
    "        label_map = {filename: int(label) for filename, label in reader}\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def get_images_paths(root, label_count):\n",
    "    dir_paths = list(map(lambda label: join(root, str(label)), range(label_count)))\n",
    "    file_paths = [list(map(lambda filename: join(dir_path, filename), os.listdir(dir_path))) for dir_path in dir_paths]\n",
    "    return roundrobin(*file_paths)\n",
    "\n",
    "\n",
    "def get_train_data(path_images, label_count, label_map):\n",
    "    paths = repeat_infinitely(get_images_paths, path_images, label_count)\n",
    "    for path in paths:\n",
    "        image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f'Image cannot be read: {path}')\n",
    "        yield (np.expand_dims(image, 0),\n",
    "               to_categorical(label_map[os.path.basename(path)], label_count)[0])\n",
    "\n",
    "\n",
    "def collect_batches(iterable, batch_size=32, randomize=False, probability=0.5, rotate=False):\n",
    "    while True:\n",
    "        images, labels = [], []\n",
    "        while len(images) < batch_size:\n",
    "            image, label = next(iterable)\n",
    "            if not randomize or np.random.rand() < probability:\n",
    "                if rotate:\n",
    "                    image = np.rot90(image, np.random.randint(0, 4), (1, 2))\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "        yield np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = read_label_map(join(PATH_DATA, 'train_labels.csv'))\n",
    "gen_train_data = get_train_data(join(PATH_DATA, 'train'), CLASS_COUNT, label_map)\n",
    "gen_train_batches = collect_batches(gen_train_data, BATCH_SIZE, randomize=True, rotate=True)\n",
    "\n",
    "gen_val_data = get_train_data(join(PATH_DATA, 'validation'), CLASS_COUNT, label_map)\n",
    "gen_val_batches = collect_batches(gen_val_data, BATCH_SIZE, randomize=False, rotate=False)\n",
    "\n",
    "# x, y = next(gen_val_batches)\n",
    "# print(x.shape)\n",
    "# print(y)\n",
    "# \n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.imshow(x[0, 0], cmap='Greys_r')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(CLASS_COUNT, is_train=True)\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(generator=gen_train_batches,\n",
    "                              steps_per_epoch=TRAIN_SIZE,\n",
    "                              epochs=20,\n",
    "                              validation_data=gen_val_data,\n",
    "                              validation_steps=VAL_SIZE,\n",
    "                              callbacks=[ReduceLROnPlateau(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_weights = join(PATH_PROJECT, 'weights')\n",
    "model.save_weights(path_weights, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_data(path_images):\n",
    "    paths = map(lambda filename: join(path_images, filename), os.listdir(path_images))\n",
    "    return np.array([list(map(lambda path: cv2.imread(path, cv2.IMREAD_GRAYSCALE), paths))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_predict = build_model(CLASS_COUNT, is_train=False)\n",
    "model_predict.load_weights(path_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_test = join(PATH_DATA, 'test')\n",
    "answers = {}\n",
    "for filename in os.listdir(path_test):\n",
    "    file_path = join(path_test, filename)\n",
    "    image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    tensor = image.reshape((1, 1, *image.shape))\n",
    "    label_categorical = model_predict.predict(tensor)\n",
    "    print(label_categorical)\n",
    "    answers[filename] = np.argmax(label_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('submission.csv', 'w') as fout:\n",
    "    for filename, label in answers.items():\n",
    "        print(filename, label, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
