{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection\n",
    "\n",
    "Hello! In this task you will create your own deep face detector.\n",
    "\n",
    "First of all, we need import some useful stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have modern Nvidia [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit)? There is your video-card model in [list](https://developer.nvidia.com/cuda-gpus) and CUDA capability >= 3.0?\n",
    "\n",
    "- Yes. You can use it for fast deep learning! In this work we recommend you use tensorflow backend with GPU. Read [installation notes](https://www.tensorflow.org/install/) with attention to gpu section, install all requirements and then install GPU version `tensorflow-gpu`.\n",
    "- No. CPU is enough for this task, but you have to use only simple model. Read [installation notes](https://www.tensorflow.org/install/) and install CPU version `tensorflow`.\n",
    "\n",
    "Of course, also you should install `keras`, `matplotlib`, `numpy` and `scikit-image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ddcf1b92dbb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/cv/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv/lib/python3.6/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Globally-importable utils.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv/lib/python3.6/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv/lib/python3.6/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown backend: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import transform\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_data import load_dataset, unpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we use processed [FDDB dataset](http://vis-www.cs.umass.edu/fddb/). Processing defined in file [./prepare_data.ipynb](prepare_data.ipynb) and consists of:\n",
    "\n",
    "1. Extract bboxes from dataset. In base dataset face defined by [ellipsis](http://vis-www.cs.umass.edu/fddb/samples/) that not very useful for basic neural network learning.\n",
    "2. Remove images with big and small faces on one shoot.\n",
    "3. Re-size images to bounding boxes (bboxes) have same size 32 +/- pixels.\n",
    "\n",
    "Each image in train, validation and test datasets have shape (176, 176, 3), but part of this image is black background. Interesting image aligned at top left corner.\n",
    "\n",
    "Bounding boxes define face in image and consist of 7 integer numbers: [image_index, min_row, min_col, max_row, max_col]. Bounding box width and height are 32 +/- 8 pixels wide.\n",
    "\n",
    "`train_bboxes` and `val_bboxes` is a list of bboxes.\n",
    "\n",
    "`train_shapes` and `val_shapes` is a list of interesting image shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run will download 30 MB data from github\n",
    "\n",
    "train_images, train_bboxes, train_shapes = load_dataset(\"train\")\n",
    "val_images, val_bboxes, val_shapes = load_dataset(\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data (1 point)\n",
    "\n",
    "Visualize first image from train dataset with face bounding box rectangle. Note that true shape of image is stored in another array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  6 27 42 54]\n"
     ]
    }
   ],
   "source": [
    "print(train_bboxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9e0e72e278>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQkAAAD8CAYAAABkQFF6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDAZDJmyRMBgMmfi/rrtoNP0XENoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e0e91eb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_idx = 0\n",
    "image = train_images[image_idx].copy()\n",
    "for idx, min_row, min_col, max_row, max_col in train_bboxes:\n",
    "    if idx == image_idx:\n",
    "        image = cv2.rectangle(image,\n",
    "                              (min_col, min_row),\n",
    "                              (max_col, max_row),\n",
    "                              (0, 0, 255),\n",
    "                              1)\n",
    "plt.imshow(image[0:train_shapes[0, 0], 0:train_shapes[0, 1], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning we should extract positive and negative samples from image.\n",
    "Positive and negative samples counts should be similar.\n",
    "Every samples should have same size.\n",
    "Positive sample is actual face resized to target shape, negative one should is some cropped not face region also resized to target shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_SHAPE = (32, 32, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to detect that your negative bounding box is not a face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scores import iou_score # https://en.wikipedia.org/wiki/Jaccard_index\n",
    "\n",
    "def is_negative_bbox(new_bbox, true_bboxes, eps=1e-1):\n",
    "    \"\"\"There bbox is 4 ints [min_row, min_col, max_row, max_col] without image index.\"\"\"\n",
    "    for bbox in true_bboxes:\n",
    "        if iou_score(new_bbox, bbox) >= eps:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_positive_negative(images, true_bboxes, image_shapes):\n",
    "    \"\"\"Retrieve positive and negative samples from image.\"\"\"\n",
    "    positive = []\n",
    "    negative = []\n",
    "    \n",
    "    # Pay attention to the fact that most part of image may be black -\n",
    "    # extract negative samples only from part [0:image_shape[0], 0:image_shape[1]]\n",
    "    \n",
    "    # Write your code here\n",
    "    # ...\n",
    "   \n",
    "    return positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_samples(images, true_bboxes, image_shapes):\n",
    "    \"\"\"Usefull samples for learning.\n",
    "    \n",
    "    X - positive and negative samples.\n",
    "    Y - one hot encoded list of zeros and ones. One is positive marker.\n",
    "    \"\"\"\n",
    "    positive, negative = get_positive_negative(images=images, true_bboxes=true_bboxes, \n",
    "                                               image_shapes=image_shapes)\n",
    "    X = positive\n",
    "    Y = [[0, 1]] * len(positive)\n",
    "    \n",
    "    X.extend(negative)\n",
    "    Y.extend([[1, 0]] * len(negative))\n",
    "    \n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract samples from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_samples(train_images, train_bboxes, train_shapes)\n",
    "X_val, Y_val = get_samples(val_images, val_bboxes, val_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There we should see faces\n",
    "from graph import visualize_samples\n",
    "visualize_samples(X_train[Y_train[:, 1] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There we shouldn't see faces\n",
    "visualize_samples(X_train[Y_train[:, 1] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier training (3 points)\n",
    "\n",
    "First of all, we should train face classifier that checks if face represented on sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image augmentation\n",
    "\n",
    "Important thing in deep learning is augmentation. Sometimes, if your model are complex and cool, you can increase quality by using good augmentation.\n",
    "\n",
    "Keras provide good [images preprocessing and augmentation](https://keras.io/preprocessing/image/). This preprocessing executes online (on the fly) while learning.\n",
    "\n",
    "Of course, if you want using samplewise and featurewise center and std normalization you should run this transformation on predict stage. But you will use this classifier to fully convolution detector, in this case such transformation quite complicated, and we don't recommend use them in classifier.\n",
    "\n",
    "For heavy augmentation you can use library [imgaug](https://github.com/aleju/imgaug). If you need, you can use this library in offline manner (simple way) and online manner (hard way). However, hard way is not so hard: you only have to write [python generator](https://wiki.python.org/moin/Generators), which returns image batches, and pass it to [fit_generator](https://keras.io/models/model/#fit_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator # Usefull thing. Read the doc.\n",
    "\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                             width_shift_range=0.2,\n",
    "                             height_shift_range=0.2,\n",
    "                             zoom_range=0.1,\n",
    "                            )\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting classifier\n",
    "\n",
    "For fitting you can use one of Keras optimizer algorithms. [Good overview](http://ruder.io/optimizing-gradient-descent/)\n",
    "\n",
    "To choose best learning rate strategy you should read about EarlyStopping and ReduceLROnPlateau or LearningRateScheduler on [callbacks](https://keras.io/callbacks/) page of keras documentation, it's very useful in deep learning.\n",
    "\n",
    "If you repeat architecture from some paper, you can find information about good optimizer algorithm and learning rate strategy in this paper. For example, every [keras application](https://keras.io/applications/) has link to paper, that describes suitable learning procedure for this specific architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "# Very usefull, pay attention\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "\n",
    "from graph import plot_history\n",
    "\n",
    "def fit(model_name, model, datagen, X_train, Y_train, X_val, Y_val, class_weight=None, epochs=10, lr=0.001, verbose=False):\n",
    "    \"\"\"Fit model.\n",
    "    \n",
    "    You can edit this function anyhow.\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "\n",
    "    model.compile(optimizer=RMSprop(lr=lr), # You can use another optimizer\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
    "                                  validation_data=(X_val, Y_val),\n",
    "                                  epochs=epochs, steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "                                  callbacks=[\n",
    "                                        # May lead to crash on windows. Uncomment and try, it may work for you\n",
    "                                        #ModelCheckpoint(model_name + \"-{epoch:02d}-{val_loss:.2f}.hdf5\", save_best_only=True)\n",
    "                                    ]\n",
    "                                 )  # starts training\n",
    "    \n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (first point out of three)\n",
    "\n",
    "We propose you make your own classification model based on lenet architecture.\n",
    "\n",
    "![lenet architecture](lenet_architecture.png)\n",
    "LeCun, Y., Bottou, L., Bengio, Y. and Haffner, P., 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), pp.2278-2324.\n",
    "\n",
    "Of course, you can use any another architecture, if want. Main thing is classification quality of your model.\n",
    "\n",
    "Acceptable validation accuracy for this task is 0.92."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, Input, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "\n",
    "# Classification model\n",
    "# You can start from LeNet architecture\n",
    "\n",
    "x = inputs = Input(shape=SAMPLE_SHAPE)\n",
    "\n",
    "# Insert your architecture here\n",
    "\n",
    "# This creates a model\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model (second point out of three)\n",
    "\n",
    "If you doesn't have fast video-card suitable for deep learning, you can first check neural network modifications with small value of parameter `epochs`, for example, 10, and then after selecting best model increase this parameter.\n",
    "Fitting on CPU can be long, we suggest do it at bedtime.\n",
    "\n",
    "Don't forget change model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_name=\"baseline\", epochs=50, model=model, datagen=datagen, X_train=X_train, X_val=X_val, Y_train=Y_train, Y_val=Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (third point out of three)\n",
    "\n",
    "After learning model weights saves in folder `data/checkpoints/`.\n",
    "Use `model.load_weights(fname)` to load best weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.load_weights(\"data/checkpoints/...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection\n",
    "\n",
    "If you have prepared classification architecture with high validation score, you can use this architecture for detection.\n",
    "\n",
    "Convert classification architecture to fully convolution neural network (FCNN), that returns heatmap of activation.\n",
    "\n",
    "### Detector model or sliding window (1 point)\n",
    "\n",
    "Now you should replace fully-connected layers with $1 \\times 1$ convolution layers.\n",
    "\n",
    "Every fully connected layer perform operation $f(Wx + b)$, where $f(\\cdot)$ is nonlinear activation function, $x$ is layer input, $W$ and $b$ is layer weights. This operation can be emulated with $1 \\times 1$ convolution with activation function $f(\\cdot)$, that perform exactly same operation $f(Wx + b)$.\n",
    "\n",
    "If there is `Flatten` layer with $n \\times k$ input size before fully connected layers, convolution should have same $n \\times k$ input size.\n",
    "Multiple fully connected layers can be replaced with convolution layers sequence.\n",
    "\n",
    "After replace all fully connected layers with convolution layers, we get fully convolution network. If input shape is equal to input size of previous network, output will have size $1 \\times 1$. But if we increase input shape, output shape automatically will be increased. For example, if convolution step of previous network strides 4 pixels, increase input size with 100 pixels along all axis makes increase output size with 25 values along all axis. We got activation map of classifier without necessary extract samples from image and multiple calculate low-level features.\n",
    "\n",
    "In total:\n",
    "1. $1 \\times 1$ convolution layer is equivalent of fully connected layer.\n",
    "2. $1 \\times 1$ convolution layers can be used to get activation map of classification network in \"sliding window\" manner.\n",
    "\n",
    "We propose replace last fully connected layer with two outputs and softmax activation function with one convolution and ReLU activation. This configuration useful to find local maximum of activation map.\n",
    "\n",
    "#### Example of replace cnn head:\n",
    "\n",
    "##### Head before convert\n",
    "\n",
    "![before replace image](before_convert.png)\n",
    "\n",
    "##### Head after convert\n",
    "\n",
    "![before replace image](after_convert.png)\n",
    "\n",
    "On this images displayed only head. `InputLayer` should be replaced with convolution part exit.\n",
    "Before convert network head takes fifty $8 \\times 8$ feature maps and returns two values: probability of negative and positive classes. This output can be considered as activation map with size $1 \\times 1$.\n",
    "\n",
    "If input have size $8 \\times 8$, output after convert would have $1 \\times 1$ size, but input size is $44 \\times 44$.\n",
    "After convert network head returns one $37 \\times 37$ activation map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FCNN\n",
    "\n",
    "IMAGE_SHAPE = (176, 176, 3)\n",
    "\n",
    "def generate_fcnn_model(image_shape):\n",
    "    \"\"\"After model compilation input size cannot be changed.\n",
    "    \n",
    "    So, we need create a function to have ability to change size later.\n",
    "    \"\"\"\n",
    "    x = inputs = Input(shape=image_shape)\n",
    "\n",
    "    # Write code here\n",
    "    # ...\n",
    "\n",
    "    # This creates a model\n",
    "    predictions = Conv2D(1, (1, 1), activation='relu')(x)\n",
    "    return Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "fcnn_model = generate_fcnn_model(IMAGE_SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1 point)\n",
    "\n",
    "Then you should write function that copy weights from classification model to fully convolution model.\n",
    "Convolution weights may be copied without modification, fully-connected layer weights should be reshaped before copy.\n",
    "\n",
    "Pay attention to last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_weights(base_model, fcnn_model):\n",
    "    \"\"\"Set FCNN weights from base model.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_fcnn_weights = []\n",
    "    prev_fcnn_weights = fcnn_model.get_weights()\n",
    "    prev_base_weights = base_model.get_weights()\n",
    "    \n",
    "    # Write code here\n",
    "    # ...\n",
    "        \n",
    "    fcnn_model.set_weights(new_fcnn_weights)\n",
    "\n",
    "copy_weights(base_model=model, fcnn_model=fcnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graph import visualize_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = fcnn_model.predict(np.array(val_images))\n",
    "visualize_heatmap(val_images, predictions[:, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detector (1 point)\n",
    "\n",
    "First detector part is getting bboxes and decision function.\n",
    "Greater decision function indicates better detector confidence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to be a little creative and implement some logic: how heatmaps will be transformed to bounding boxes. In you fcnn_model output you have heatmap of some size. You need to understand how heatmap coordinates (predictions) correlate with detecting image on the source. For example, if your model uses two poolings you need to multiply coordinates by 4.\n",
    "After that you need to get the biggest signals for most certain locations of face. Hint: peak_local_max function can be useful.\n",
    "Note also that evaluation process will pick the best face you detected near real one and all others will be false positives. So to improve your score you better choose just one (maybe best) face in a region of local max activations. You can also use any clusterization mechanism or non-maximum suppression technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Detection\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "def get_bboxes_and_decision_function(fcnn_model, images, image_shapes):\n",
    "    cropped_images = np.array([transform.resize(image, IMAGE_SHAPE, mode=\"reflect\") for image in images])\n",
    "    pred_bboxes, decision_function = [], []\n",
    "   \n",
    "    # Predict\n",
    "    predictions = fcnn_model.predict(cropped_images)\n",
    "\n",
    "    # Write code here\n",
    "    # ...\n",
    "        \n",
    "    return pred_bboxes, decision_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detector visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graph import visualize_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_bboxes, decision_function = get_bboxes_and_decision_function(fcnn_model=fcnn_model, images=val_images, image_shapes=val_shapes)\n",
    "\n",
    "visualize_bboxes(images=val_images,\n",
    "                 pred_bboxes=pred_bboxes,\n",
    "                 true_bboxes=val_bboxes,\n",
    "                 decision_function=decision_function\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detector score (1 point)\n",
    "\n",
    "Write [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) graph.\n",
    "\n",
    "You can use function `best_match` to extract matching between prediction and ground truth, false positive and false negative samples. Pseudo-code for calculation precision and recall graph:\n",
    "    \n",
    "    # Initialization for first step threshold = -inf\n",
    "    tp, fp := 0, 0 # We haven't any positive sample\n",
    "    fn := |true_bboxes| # All true bboxes haven't been caught\n",
    "    tn := |false_positive| # But also all false positives samples haven't been caught if threshold = -inf\n",
    "    \n",
    "    Sort pred_bboxes with order defined by decision_function\n",
    "    y_true := List of answers for \"Is the bbox have matching in y_true?\" for every bbox in pred_bboxes\n",
    "    \n",
    "    for y_on_this_step in y_true:\n",
    "        # Now we increase threshold, so some predicted bboxes makes positive.\n",
    "        # If y_t is True then the bbox is true positive else bbox is false positive\n",
    "        # So we should\n",
    "        Update tp, tn, fp, fn with attention to y_on_this_step\n",
    "        \n",
    "        Add precision and recall point calculated by formula through tp, tn, fp, fn on this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scores import best_match\n",
    "from graph import plot_precision_recall\n",
    "\n",
    "def precision_recall_curve(pred_bboxes, true_bboxes, decision_function):\n",
    "    precision, recall = [], []\n",
    "    \n",
    "    # Write code here\n",
    "    # ...\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision, recall = precision_recall_curve(pred_bboxes=pred_bboxes, true_bboxes=val_bboxes, decision_function=decision_function)\n",
    "plot_precision_recall(precision=precision, recall=recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold (1 point)\n",
    "\n",
    "Next step in detector creating is select threshold for decision_function.\n",
    "Every possible threshold presents point on recall-precision graph.\n",
    "\n",
    "Select threshold for `recall=0.85`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 5.9\n",
    "\n",
    "def detect(fcnn_model, images, image_shapes, threshold=THRESHOLD, return_decision=True):\n",
    "    \"\"\"Get bboxes with decision_function not less then threshold.\"\"\"\n",
    "    pred_bboxes, decision_function = get_bboxes_and_decision_function(fcnn_model, images, image_shapes)   \n",
    "    result, result_decision = [], []\n",
    "    \n",
    "    # Write code here\n",
    "    # ...\n",
    "    \n",
    "    if return_decision:\n",
    "        return result, result_decision\n",
    "    else:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_bboxes, decision_function = detect(fcnn_model=fcnn_model, images=val_images, image_shapes=val_shapes, return_decision=True)\n",
    "\n",
    "visualize_bboxes(images=val_images,\n",
    "                 pred_bboxes=pred_bboxes,\n",
    "                 true_bboxes=val_bboxes,\n",
    "                 decision_function=decision_function\n",
    "                )\n",
    "\n",
    "precision, recall = precision_recall_curve(pred_bboxes=pred_bboxes, true_bboxes=val_bboxes, decision_function=decision_function)\n",
    "plot_precision_recall(precision=precision, recall=recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset (1 point)\n",
    "\n",
    "Last detector preparation step is testing.\n",
    "\n",
    "Attention: to avoid over-fitting, after testing algorithm you should run [./prepare_data.ipynb](prepare_data.ipynb), and start all fitting from beginning.\n",
    "\n",
    "Detection score (in graph header) should be 0.75 or greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_images, test_bboxes, test_shapes = load_dataset(\"test\")\n",
    "pred_bboxes, decision_function = detect(fcnn_model=fcnn_model, images=test_images, image_shapes=test_shapes, return_decision=True)\n",
    "visualize_bboxes(images=test_images,\n",
    "                 pred_bboxes=pred_bboxes,\n",
    "                 true_bboxes=test_bboxes,\n",
    "                 decision_function=decision_function\n",
    "                )\n",
    "\n",
    "precision, recall = precision_recall_curve(pred_bboxes=pred_bboxes, true_bboxes=test_bboxes, decision_function=decision_function)\n",
    "plot_precision_recall(precision=precision, recall=recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional tasks\n",
    "\n",
    "### Real image dataset\n",
    "\n",
    "Test your algorithm on original (not scaled) data.\n",
    "Visualize bboxes and plot precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First run will download 523 MB data from github\n",
    "\n",
    "original_images, original_bboxes, original_shapes = load_dataset(\"original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard negative mining\n",
    "\n",
    "Upgrade the score with hard negative mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hard_negative(train_images, image_shapes, train_bboxes, X_val, Y_val, base_model, fcnn_model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hard_negative(train_images=train_images, image_shapes=train_shapes, train_bboxes=train_bboxes, X_val=X_val, Y_val=Y_val, base_model=model, fcnn_model=fcnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.load_weights(\"data/checkpoints/...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_weights(base_model=model, fcnn_model=fcnn_model)\n",
    "\n",
    "pred_bboxes, decision_function = get_bboxes_and_decision_function(fcnn_model=fcnn_model, images=val_images, image_shapes=val_shapes)\n",
    "\n",
    "visualize_bboxes(images=val_images,\n",
    "                 pred_bboxes=pred_bboxes,\n",
    "                 true_bboxes=val_bboxes,\n",
    "                 decision_function=decision_function\n",
    "                )\n",
    "\n",
    "precision, recall = precision_recall_curve(pred_bboxes=pred_bboxes, true_bboxes=val_bboxes, decision_function=decision_function)\n",
    "plot_precision_recall(precision=precision, recall=recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Multi scale detector\n",
    "\n",
    "Write and test detector with [pyramid representation][pyramid].\n",
    "[pyramid]: https://en.wikipedia.org/wiki/Pyramid_(image_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiscale_detector(fcnn_model, images, image_shapes):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Next  step\n",
    "\n",
    "Next steps in deep learning detection are R-CNN, Faster R-CNN and SSD architectures.\n",
    "This architecture realization is quite complex.\n",
    "For this reason the task doesn't cover them, but you can find the articles in the internet."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
