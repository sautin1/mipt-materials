# Описание решений
## baseline
Базовое решение на основе [кода](http://norvig.com/spell-correct.html) Peter Norvig.
Не использует модели FastTest.


## fasttext_plain
Использована только маленькая модель FastText.
Для каждого базового слова находим K самых близких слов-кандидатов согласно их векторным представлениям. Среди них выбираем кандидата с максимальным произведением частоты его встречаемости в тексте `data/big.txt` и расстоянием до базового слова.

* механизм выбора (selection mechanism) -- выбираем кандидата с наибольшей комбинированной вероятностью;
* модель выбора кандидатов (candidate model) -- 20 слов, чье векторное представление наиболее близко представлению базового слова;
* языковая модель (language model) -- частота слова в тексте `data/big.txt`;
* модель ошибки (error model) -- расстояние между вектором слова-кандидата и вектором базового слова.

Решение получило низкий score за счет того, что среди кандидатов было немало принципиально других (отличных от базового) слов (но близких по смыслу), а также за счет того, что степень "сходства" кандидата и базового слова определялась как близость в векторном пространстве (т.е. скорее близость по смыслу слов, а не по написанию).

## similarity
*Идея решения была подсказана Владом Корзуном.*

Использовано базовое решение и маленькая модель FastText.
Для каждого базового слова составляем множество кандидатов так же, как и в базовом решении. Если базовое слово есть в словаре модели FastText, то выбираем кандидата с наименьшим расстоянием (similarity, в векторном смысле) до базового слова. В противном случае выбираем того кандидата, которого выдает базовое решение.

* механизм выбора (selection mechanism) -- выбираем кандидата с наибольшей комбинированной вероятностью;
* модель выбора кандидатов (candidate model) -- аналогична базовому алгоритму;
* языковая модель (language model) -- расстояние до базового слова или частота слова в тексте `data/big.txt`;
* модель ошибки (error model) -- аналогична базовому алгоритму.

## ranking
*Идея решения была найдена на [kaggle](https://www.kaggle.com/cpmpml/spell-checker-using-word2vec/code).*

Использовано базовое решение и маленькая модель FastText.
Для каждого базового слова составляем множество кандидатов так же, как и в базовом решении. Из них выбираем того кандидата, индекс которого в словаре модели меньше (словарь модели отсортирован по частоте встречаемости слова в обучающей выборке). 

* механизм выбора (selection mechanism) -- выбираем кандидата с наибольшей комбинированной вероятностью;
* модель выбора кандидатов (candidate model) -- аналогична базовому алгоритму, только множество кандидатов ограничено не словарем *data/big.txt*, а словарем модели;
* языковая модель (language model) -- (1 - rank) / 999999, где rank -- ранг (индекс) кандидата в словаре модели, 999999 -- количество слов в словаре модели;
* модель ошибки (error model) -- аналогична базовому алгоритму (при этом множество кандидатов с редакторским расстоянием 1 строится даже в том случае, когда базовое слово принадлежит словарю модели).

# Сравнение результатов
В таблице представлены результаты замера качества всех решений на датасетах *data/spell-testset1.txt* и *data/spell-testset2.txt* по метрике accuracy.

|                | baseline | fasttext_plain | similarity | ranking   |
|----------------|----------|----------------|------------|-----------|
| spell-testset1 | 0.748    | 0.207          | **0.789**  | 0.7       |
| spell-testset2 | 0.675    | 0.210          | 0.693      | **0.695** |

# Контекст
В условиях возможности использования контекста базового слова (и большой модели FastText, хранящей в себе векторы в том числе и для различных символьных n-грамм) задачу поиска и исправления опечаток можно было бы решать следующими способами:
1. (с использованием большой модели, без использования контекста) Найти в базовом слове редкие n-граммы (пользуясь словарем модели), заменить их поочередно и в совокупности на все возможные и более вероятные исправления этих n-грамм (с редакторским расстоянием 1 и 2). Из полученных слов-кандидатов оставить только те, которые есть в словаре модели или словаре *data/big.txt*. Наконец, выбрать кандидата с наибольшей частотой или кандидата, у которого произведение частот всех его n-грамм наибольшее.
2. Для получения кандидатов воспользоваться методом `most_similar` модели, передав в качестве аргумента `positive` список слов контекста.
